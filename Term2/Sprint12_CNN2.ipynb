{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ 畳み込みニューラルネットワーク２\n",
    "## 2次元の畳み込みニューラルネットワークスクラッチ\n",
    "2次元に対応した畳み込みニューラルネットワーク（CNN）のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "プーリング層なども作成することで、CNNの基本形を完成させます。クラスの名前はScratch2dCNNClassifierとしてください。\n",
    "\n",
    "## データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "(n_samples, n_channels, height, width)のNCHWまたは(n_samples, height, width, n_channels)のNHWCどちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:06.568859Z",
     "start_time": "2019-10-25T18:21:01.707204Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:07.491144Z",
     "start_time": "2019-10-25T18:21:06.641184Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mini_shape (1000, 1, 28, 28)\n",
      "X_val_shape (9000, 1, 28, 28)\n",
      "y_mini_shape (1000,)\n",
      "y_mini_test_shape (9000,)\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータのダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#チャネル軸の追加\n",
    "X_train = X_train.reshape(-1,1,28,28)\n",
    "X_test = X_test.reshape(-1,1,28,28)\n",
    "# 型の変換\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "#正規化？\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 仮データ作成\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_mini_train, X_mini, y_mini_train, y_mini = train_test_split(X_test, y_test, test_size=0.1)\n",
    "print('X_mini_shape',X_mini.shape)\n",
    "print('X_val_shape',X_mini_train.shape)\n",
    "print('y_mini_shape',y_mini.shape)\n",
    "print('y_mini_test_shape',y_mini_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを2次元にする関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:07.679644Z",
     "start_time": "2019-10-25T18:21:07.669664Z"
    }
   },
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    #入力データの（データ数, チャンネル, 高さ, 幅）\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    #パディング\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "    #print(col.shape)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:07.862151Z",
     "start_time": "2019-10-25T18:21:07.854171Z"
    }
   },
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:08.563667Z",
     "start_time": "2019-10-25T18:21:08.559678Z"
    }
   },
   "outputs": [],
   "source": [
    "#以後X_mini，y_miniを仮データとし，確認していく\n",
    "N,C,H,W = X_mini.shape\n",
    "stride = 1\n",
    "pad = 0\n",
    "FH = 3\n",
    "FW = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:09.718511Z",
     "start_time": "2019-10-25T18:21:09.499132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676000, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=im2col(input_data=X_mini,filter_h=FH,filter_w=FW)\n",
    "col.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】2次元畳み込み層の作成\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "フォワードプロパゲーションの数式は以下のようになります。               \n",
    "$$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$$                \n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、mチャンネルの値  \n",
    "i : 配列の行方向のインデックス  \n",
    "j : 配列の列方向のインデックス  \n",
    "m : 出力チャンネルのインデックス  \n",
    "K : 入力チャンネル数  \n",
    "$F_h,F_w$ : 高さ方向（h）と幅方向（w）のフィルタのサイズ  \n",
    "$x_{(i+s),(j+t),k}$ : 入力の配列の(i+s)行(j+t)列、kチャンネルの値  \n",
    "$w_{s,t,k,m}$ : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み  \n",
    "$b_m$ : mチャンネルへの出力のバイアス項  \n",
    "全てスカラーです。  \n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。            \n",
    "$$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$$           \n",
    "$α$ : 学習率  \n",
    "$\\frac{∂L}{∂w_{s,t,k,m}} : w_{s,t,k,m}$ に関する損失 L の勾配  \n",
    "$\\frac{∂L}{∂b_m} : b_m$ に関する損失 L の勾配\n",
    "勾配 $\\frac{∂L}{∂w_{s,t,k,m}} や \\frac{∂L}{∂b_m}$ を求めるためのバックプロパゲーションの数式が以下である。     \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    "$$                      \n",
    "$\\frac{∂L}{∂a_i}$ : 勾配の配列のi行j列、mチャンネルの値  \n",
    "$N_{out,h},N_{out,w}$ : 高さ方向（h）と幅方向（w）の出力のサイズ  \n",
    "前の層に流す誤差の数式は以下です。                      \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$$                \n",
    "$\\frac{∂L}{∂x_{i,j,k}} $: 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "M : 出力チャンネル数  \n",
    "ただし、$i−s<0 または i−s>N_{out,h}−1 または j−t<0または j−t>N_{out,w}−1 のとき\\frac{∂L}{∂a_{(i−s),(j−t),m}}=0 です。$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:14.252744Z",
     "start_time": "2019-10-25T18:21:14.238781Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \"\"\"\n",
    "    2次元畳み込み層クラス\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    FN : int 出力チャンネル数（フィルターの個数）\n",
    "    FH : int フィルターの高さ\n",
    "    FW : int フィルターの幅\n",
    "    S : int (default: 1)ストライド\n",
    "    P : int (default: 0)パディング\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'conv'\n",
    "      layerの種類\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,FN=3, FH=3,FW=3, pad=0, stride=1):\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する           \n",
    "        self.FH = FH\n",
    "        self.FW = FW\n",
    "        self.FN = FN\n",
    "        self.pad = pad\n",
    "        self.S = stride\n",
    "        self.group = 'conv'\n",
    "        \n",
    "    def initialize(self, input_dim,summary, init_type, optimizer, sigma=1e-2, lr=1e-2):\n",
    "        \"\"\"\n",
    "        W,B,initializer,optimizerを初期化するメソッド\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        input_dim :次の形のtuple, (入力チャンネル,高さ,横幅)\n",
    "            入力サイズ\n",
    "        summary: bool\n",
    "            Trueにするとshapeを出力\n",
    "        initializer: class \n",
    "            initializerのクラス\n",
    "        optimizer: \n",
    "            optimizerのクラス\n",
    "        sigma:float\n",
    "            Simpleinitializerを選んだ時のパラメータ\n",
    "            \n",
    "        Return\n",
    "        ----------\n",
    "        out_dim:次の形のtuple, (出力チャンネル,OH,OW)\n",
    "        \"\"\"\n",
    "        #出力サイズを計算する。\n",
    "        C, H, W = input_dim\n",
    "        \n",
    "        self.OH, self.OW = out_calc(H,W, self.pad, self.FH, self.FW, self.S)\n",
    "        out_dim = (self.FN, self.OH, self.OW)\n",
    "        \n",
    "        #初期値を設定する。\n",
    "        initializer = Initializer(init_type, C * self.FH * self.FW, sigma)\n",
    "        self.W = initializer.W(self.FN, C, self.FH, self.FW)\n",
    "        self.B = initializer.B(self.FN)\n",
    "        if summary:\n",
    "            print(self.group,'layer shape={}, param={}'.format(out_dim, self.FH*self.FW*C*self.FN+self.FN))\n",
    "                \n",
    "        #optimizerを設定する。\n",
    "        self.optimizer = optimizer(lr)\n",
    "        return out_dim \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, self.C, H, W = x.shape\n",
    "\n",
    "        col = im2col(x, self.FH, self.FW, self.S, self.pad)\n",
    "        col_W = self.W.reshape(self.FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.B\n",
    "        out = out.reshape(N, self.OH, self.OW, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, self.FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(self.FN, self.C, self.FH, self.FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, self.FH, self.FW, self.S, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題2】2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。               \n",
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1\n",
    "$$           \n",
    "$N_{out} : 出力のサイズ（特徴量の数）\\\\\n",
    "N_{in} : 入力のサイズ（特徴量の数）\\\\\n",
    "P : ある方向へのパディングの数\\\\\n",
    "F : フィルタのサイズ\\\\\n",
    "S : ストライドのサイズ\\\\\n",
    "h が高さ方向、w が幅方向である$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:15.277006Z",
     "start_time": "2019-10-25T18:21:15.274006Z"
    }
   },
   "outputs": [],
   "source": [
    "def out_calc(H,W, pad, FH, FW, stride):\n",
    "    OH = ((H  + 2*pad - FH) / stride) + 1\n",
    "    OW = ((W + 2*pad - FW) / stride) + 1\n",
    "    return int(OH), int(OW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。               \n",
    "$$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$$           \n",
    "$P_{i,j} : i行j列への出力する場合の入力配列のインデックスの集合。 S_h×S_w の範囲内の行（p）と列（q）\\\\\n",
    "S_h,S_w : 高さ方向（h）と幅方向（w）のストライドのサイズ\\\\\n",
    "(p,q)∈P_{i,j} : P_{i,j} に含まれる行（p）と列（q）のインデックス\\\\\n",
    "a_{i,j,m} : 出力される配列のi行j列、kチャンネルの値\\\\\n",
    "x_{p,q,k} : 入力の配列のp行q列、kチャンネルの値\\\\\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\\\\\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス  \n",
    "(p,q) を保持しておく必要があります。\\\\\n",
    "フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。  \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:16.253275Z",
     "start_time": "2019-10-25T18:21:16.240310Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    最大プーリング層のクラス\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    PH : int\n",
    "      フィルターの高さ\n",
    "    PW : int\n",
    "      フィルターの幅\n",
    "    S : int (default: 1)\n",
    "      ストライド\n",
    "    P : int (default: 0)\n",
    "      パディング\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'pooling'\n",
    "      layerの種類\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, PH=2, PW=2, P=0, S=2):\n",
    "        self.PH = PH\n",
    "        self.PW = PW\n",
    "        self.S = S\n",
    "        self.pad = P\n",
    "        self.arg_max = None\n",
    "        self.group = 'pooling'\n",
    "        \n",
    "    def initialize(self, input_dim,summary, *args, **kargs):\n",
    "        \"\"\"\n",
    "        出力サイズを出力するメソッド\n",
    "        [*args, **kargs]はダミー\n",
    "        Return\n",
    "        ----------\n",
    "        out_dim:次の形のtuple, (出力チャンネル,OH,OW)\n",
    "        \"\"\"\n",
    "        #出力サイズを計算する。\n",
    "        C, H, W = input_dim\n",
    "        self.OH, self.OW = out_calc(H,W, self.pad, self.PH, self.PW, self.S)\n",
    "        #OH = out_calc(H, self.P, self.PH, self.S)\n",
    "        #OW = out_calc(W, self.P, self.PW, self.S)\n",
    "        out_dim = (C, self.OH, self.OW)\n",
    "        if summary:\n",
    "            print(self.group,'layer shape=',out_dim)\n",
    "            \n",
    "        return out_dim\n",
    "  \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        col = im2col(x, self.PH, self.PW, self.S, self.pad)\n",
    "        col = col.reshape(-1, self.PH*self.PW)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, self.OH, self.OW, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.PH * self.PW\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.PH, self.PW, self.S, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:17.033609Z",
     "start_time": "2019-10-25T18:21:17.026639Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \"\"\"\n",
    "    平滑化するクラス\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'flatten'\n",
    "      layerの種類\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化    \n",
    "        self.group = 'flatten'\n",
    "        \n",
    "    def initialize(self, input_dim,summary, *args, **kargs):\n",
    "        \"\"\"\n",
    "        出力サイズを出力するメソッド\n",
    "        [*args, **kargs]はダミー    \n",
    "        Return\n",
    "        ----------\n",
    "        out_dim:次の形のtuple, (出力チャンネル,OH,OW)\n",
    "        \"\"\"\n",
    "        #出力サイズを計算する。\n",
    "        C, H, W = input_dim\n",
    "        if summary:\n",
    "            print(self.group,'layer shape=', (C*H*W))\n",
    "\n",
    "        return C*H*W\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (N,C,高さ,横幅)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (N、C*高さ*横幅)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X_shape = X.shape #バックワードで使用\n",
    "        out = X.reshape(len(X), -1) #サンプルサイズだけ残してflatへ\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dout: 次の形のndarray, shape(N, n_nodes)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (N、C,高さ,横幅)\n",
    "            出力\n",
    "        \"\"\"\n",
    "                \n",
    "        dout = dout.reshape(self.X_shape)\n",
    "        return dout\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全結合層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:18.174639Z",
     "start_time": "2019-10-25T18:21:18.166657Z"
    }
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数pre_nodesからnodesへの全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : int\n",
    "      層のノード数\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'FC'\n",
    "      layerの種類\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes):\n",
    "        self.nodes = nodes\n",
    "        self.group = 'FC'\n",
    "        \n",
    "    def initialize(self, pre_nodes, summary, init_type, optimizer, sigma=1e-2, lr=1e-2):\n",
    "        \"\"\"\n",
    "        重み、バイアスを初期化して出力数を渡してあげる\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim :次の形のtuple, (入力チャンネル,高さ,横幅)\n",
    "            入力サイズ\n",
    "        initializer: class\n",
    "            initializerのクラス\n",
    "        optimizer: class\n",
    "            optimizerのクラス\n",
    "        lr : float(1e-2)\n",
    "            optimizerに渡す学習率\n",
    "        sigma : float(1e-2)\n",
    "            Simpleinitializerを選んだ時のパラメータ\n",
    "        \"\"\"\n",
    "        \n",
    "        #初期値を設定する。\n",
    "        initializer = Initializer(init_type, pre_nodes, sigma)\n",
    "        self.W = initializer.W(pre_nodes, self.nodes)\n",
    "        self.B = initializer.B(self.nodes)\n",
    "        \n",
    "        if summary:\n",
    "            print(self.group,'layer shape={}, param={}'.format(self.W.shape,pre_nodes*self.nodes+self.nodes))\n",
    "\n",
    "        #optimizerを設定する。\n",
    "        self.optimizer = optimizer(lr)\n",
    "        \n",
    "        return self.nodes\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, pre_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, nodes)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.Z = X\n",
    "        A = X @ self.W + self.B\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, pre_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = dA\n",
    "        self.dW = self.Z.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        # 重みを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適化手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:19.020081Z",
     "start_time": "2019-10-25T18:21:19.016093Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W -= self.lr * (layer.dW / layer.dB.shape[0])\n",
    "        layer.B -= self.lr * np.mean(layer.dB, axis=0)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数\n",
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:19.871492Z",
     "start_time": "2019-10-25T18:21:19.866502Z"
    }
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"\n",
    "    ReLU関数の活性化関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.group = 'activation'\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーションのときのメソッド\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 全結合後の行列 shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        self.mask = A <= 0\n",
    "        Z = A.copy()\n",
    "        Z[self.mask] = 0\n",
    "        return  Z\n",
    "\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 全結合後の行列shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        dZ[self.mask] = 0\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:20.800544Z",
     "start_time": "2019-10-25T18:21:20.793563Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数の活性化関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "        self.entropy = None # バッチ単位でのエントロピー\n",
    "        self.group = 'activation'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーションのときのメソッド\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 全結合後の行列 shape(batch_size, pre_nodes)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 活性化後の行列　shape(batch_size,  pre_nodes)\n",
    "        \"\"\"\n",
    "        c = np.max(A, axis=1,keepdims=True)\n",
    "        self.Z = np.exp(A-c) / np.sum(np.exp(A-c), axis=1).reshape(-1, 1)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        \"\"\"\n",
    "        バックワードと交差エントロピーを計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 全結合後の行列 shape(batch_size, pre_nodes)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 活性化後の行列　shape(batch_size,  pre_nodes)\n",
    "        \"\"\"\n",
    "        #勾配はこっち\n",
    "        dA = self.Z - Y\n",
    "        return dA\n",
    "    \n",
    "    def loss(self, Y):\n",
    "        entropy = -np.sum(Y * np.log(self.Z + 1e-5), axis=1) #サンプル毎のエントロピー(batch_size,)\n",
    "        entropy = entropy.sum() / len(entropy)  # スカラー\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:21.833311Z",
     "start_time": "2019-10-25T18:21:21.822332Z"
    }
   },
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期値設定\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, init_type, pre_nodes, sigma):\n",
    "        if init_type == 'simple':\n",
    "            self.sigma = sigma\n",
    "        elif init_type == 'Xavier':\n",
    "            self.sigma = 1 / np.sqrt(pre_nodes)\n",
    "        elif init_type == 'He':\n",
    "            self.sigma = np.sqrt(2 / pre_nodes)\n",
    "\n",
    "    def W(self,*args):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : int\n",
    "          ノード数や、チャンネル数等必要なサイズを入力\n",
    "        Returns\n",
    "        ----------\n",
    "        W :次の形のndarray, shape (args)\n",
    "            重み\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.standard_normal(size=args)\n",
    "        return W\n",
    "    def B(self, *args):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : int\n",
    "          ノード数等を入力。入力した形の必要なサイズを入力\n",
    "        Returns\n",
    "        ----------\n",
    "        B :次の形のndarray, shape (args)\n",
    "            バイアス\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.standard_normal(size=args)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:22.856912Z",
     "start_time": "2019-10-25T18:21:22.848958Z"
    }
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:23.477362Z",
     "start_time": "2019-10-25T18:21:23.459397Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scratch2dCNNClassifier:\n",
    "    \"\"\"\n",
    "    ディープなニューラルネットワーク分類器\n",
    "    層を増やすことが出来る。\n",
    "    バッチをランダムで抽出する。\n",
    "    エポック毎にバッチを取り直すことも可能。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int　(30)\n",
    "        バッチサイズ\n",
    "    n_epoch : int (100)\n",
    "        エポック数\n",
    "    e_threshold : float(1e-2)\n",
    "        エポック途中終了の為のエントロピーの閾値\n",
    "    n_iter : int(1000)\n",
    "        1エポック辺りのイテレーション数\n",
    "    repeat_batch_process : bool(True)\n",
    "        Trueの場合１エポック毎にバッチをランダムに取り直す。\n",
    "    restore_extraction:bool(True)\n",
    "        学習するバッチをランダム抽出する際に復元か、非復元か選ぶ。基本は復元(ブートストラップ)\n",
    "    seed : int(0)\n",
    "        ランダムシード\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    entropy : shape(n_epoch, n_iter)\n",
    "        1バッチごとのエントロピー\n",
    "    layers : list\n",
    "        layerのリスト\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,batch_size=30,n_epochs=100,\n",
    "                 e_threshold=1e-2,n_iter=1000,repeat_batch_process=True,restore_extraction=True,seed=0):\n",
    "        self._n_iter = n_iter\n",
    "        self._repeat_batch_process = repeat_batch_process\n",
    "        self._restore_extraction = restore_extraction\n",
    "        self._batch_size = batch_size\n",
    "        self._n_epochs = n_epochs\n",
    "        self._e_threshold = e_threshold  # 誤差の閾値        \n",
    "        self.entropy = None\n",
    "        self.epoch_entropy_mean = None\n",
    "                            \n",
    "    def sequential(self,*layers):\n",
    "        \"\"\"\n",
    "        layerをつなげるメソッド。\n",
    "        \"\"\"\n",
    "        self.layers = []        \n",
    "        for layer in layers:\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def initialize(self, input_dim, summary, init_type, optimizer, sigma=1e-2, lr=1e-2):\n",
    "        \"\"\"\n",
    "        それぞれのlayerの初期化メソッド\n",
    "        活性化層以外の層のinitializeメソッドを使う。\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if layer.group != 'activation':\n",
    "                input_dim = layer.initialize(input_dim,summary, init_type, optimizer, sigma=sigma, lr=lr)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, validation_split=0.1):\n",
    "        \"\"\"\n",
    "        NNを学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, チャンネル数,高さ,幅)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, クラス)\n",
    "            学習用データの正解値\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # trainとvalにデータを分ける。        \n",
    "        split_index = int(X.shape[0] * validation_split)\n",
    "        X_train = X[split_index:]\n",
    "        y_train = y[split_index:]\n",
    "        X_val = X[:split_index]\n",
    "        y_val = y[:split_index]\n",
    "        \n",
    "        # yの値をワンホットエンコーディングする。        \n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "        y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "        \n",
    "\n",
    "        # リピートしない場合はここでミニバッチ化\n",
    "        if self._repeat_batch_process == False:\n",
    "            train_batch = GetMiniBatch(X_train, y_train_one_hot,\n",
    "                                       batch_size=self._batch_size) \n",
    "        # バッチをランダム取得するためのindexを取得\n",
    "        batch_index = np.random.choice(int(X_train.shape[0] / self._batch_size),\n",
    "                                       self._n_iter,\n",
    "                                       replace=self._restore_extraction)  # n_iterのindex\n",
    "        self.train_loss = []  # 1エポックの平均のlossの入れ物\n",
    "        self.val_loss = []  # 1エポックの平均のlossの入れ物\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "        # 学習開始\n",
    "        for epoch in range(self._n_epochs):\n",
    "            if self._repeat_batch_process:\n",
    "                train_batch = GetMiniBatch(\n",
    "                    X_train, y_train_one_hot, batch_size=self._batch_size)  # バッチに分ける。\n",
    "            for i, index in enumerate(batch_index):\n",
    "                X_batch, y_batch = train_batch[index][0].copy(\n",
    "                ), train_batch[index][1].copy()\n",
    "                # フォワードプロパゲーション\n",
    "                for layer_index in range(len(self.layers)):\n",
    "                    X_batch = self.layers[layer_index].forward(X_batch)\n",
    "                # バックプロパゲーション\n",
    "                for layer_index in range(1, len(self.layers) + 1):\n",
    "                    y_batch = self.layers[-layer_index].backward(y_batch)\n",
    "            \n",
    "            # エポックごとにlossとaccを計算\n",
    "            tr_pred = self.predict(X_train)\n",
    "            tr_loss = self.layers[-1].loss(y_train_one_hot)\n",
    "            val_pred = self.predict(X_val)\n",
    "            val_loss = self.layers[-1].loss(y_val_one_hot)\n",
    "            tr_acc = np.sum(y_train == tr_pred) / X_train.shape[0]\n",
    "            val_acc = np.sum(y_val == val_pred) / X_val.shape[0]\n",
    "            self.train_loss.append(tr_loss)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.train_acc.append(tr_acc)\n",
    "            self.val_acc.append(val_acc)\n",
    "            print('{}エポック目のloss= {:.5f}'.format(epoch,tr_loss))\n",
    "            # 誤差が閾値以下になったらエポック終了\n",
    "            if tr_loss < self._e_threshold:  \n",
    "                print('lossが{:.3f}より低いよ！'.format(self._e_threshold))\n",
    "                break\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        NNで予測する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, チャンネル数,高さ,幅)\n",
    "            学習用データの特徴量        \n",
    "        \"\"\"\n",
    "\n",
    "        # フォワードプロパゲーション\n",
    "        out = X.copy()\n",
    "        for layer in range(len(self.layers)):\n",
    "            out = self.layers[layer].forward(out)\n",
    "\n",
    "        return np.argmax(out, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:24.124153Z",
     "start_time": "2019-10-25T18:21:24.120139Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn2d = Scratch2dCNNClassifier(batch_size=32,\n",
    "                                n_epochs=5,\n",
    "                                n_iter=100,\n",
    "                                repeat_batch_process=False,\n",
    "                                restore_extraction=False,\n",
    "                                seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:21:25.080064Z",
     "start_time": "2019-10-25T18:21:25.056163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer shape=(6, 30, 30), param=60\n",
      "pooling layer shape= (6, 16, 16)\n",
      "flatten layer shape= 1536\n",
      "FC layer shape=(1536, 128), param=196736\n",
      "FC layer shape=(128, 50), param=6450\n",
      "FC layer shape=(50, 10), param=510\n"
     ]
    }
   ],
   "source": [
    "cnn2d.sequential(\n",
    "    Conv2d(FN=6,FH=3, FW=3,  pad=2, stride=1), \n",
    "    Relu(),\n",
    "    MaxPool2D(PH=2, PW=2, P=1,S=2),\n",
    "    Flatten(), \n",
    "    FC(128),\n",
    "    Relu(),\n",
    "    FC(50), \n",
    "    Relu(),\n",
    "    FC(10), \n",
    "    Softmax()\n",
    ")\n",
    "\n",
    "input_dim = (1, 28, 28)\n",
    "cnn2d.initialize(input_dim, summary=True, init_type='He', optimizer=SGD, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:24:51.728360Z",
     "start_time": "2019-10-25T18:24:06.757386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1281\n",
      "0エポック目のloss= 0.55245\n",
      "1エポック目のloss= 0.44397\n",
      "2エポック目のloss= 0.39138\n",
      "3エポック目のloss= 0.35950\n",
      "4エポック目のloss= 0.33625\n"
     ]
    }
   ],
   "source": [
    "#学習\n",
    "cnn2d.fit(X_mini_train, y_mini_train,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:28:27.770626Z",
     "start_time": "2019-10-25T18:28:27.184441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.887\n"
     ]
    }
   ],
   "source": [
    "# 推定\n",
    "y_pred = cnn2d.predict(X_mini)\n",
    "acc = accuracy_score(y_pred,y_mini)\n",
    "print('accuracy_score:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "１．              \n",
    "* 入力サイズ : 144×144, 3チャンネル\n",
    "* フィルタサイズ : 3×3, 6チャンネル\n",
    "* ストライド : 1\n",
    "* パディング : なし\n",
    "                  \n",
    "２．                             \n",
    "* 入力サイズ : 60×60, 24チャンネル\n",
    "* フィルタサイズ : 3×3, 48チャンネル\n",
    "* ストライド　: 1\n",
    "* パディング : なし\n",
    "         \n",
    "３．        \n",
    "* 入力サイズ : 20×20, 10チャンネル\n",
    "* フィルタサイズ: 3×3, 20チャンネル\n",
    "* ストライド : 2\n",
    "* パディング : なし\n",
    "           \n",
    "最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:29:03.742850Z",
     "start_time": "2019-10-25T18:29:03.736859Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn2d = Scratch2dCNNClassifier(batch_size=100,n_epochs=5,n_iter=500,\n",
    "                                repeat_batch_process=False,\n",
    "                                restore_extraction=False,\n",
    "                                seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:29:59.665669Z",
     "start_time": "2019-10-25T18:29:59.661693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer shape=(6, 142, 142), param=168\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "cnn2d.sequential(Conv2d(FN=6,FH=3, FW=3,  pad=0, stride=1))\n",
    "\n",
    "input_dim = (3, 144, 144)\n",
    "cnn2d.initialize(input_dim, summary=True, init_type='He', optimizer=SGD, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:31:05.597840Z",
     "start_time": "2019-10-25T18:31:05.593855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer shape=(48, 58, 58), param=10416\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "cnn2d.sequential(Conv2d(FN=48,FH=3, FW=3,  pad=0, stride=1))\n",
    "\n",
    "input_dim = (24, 60, 60)\n",
    "cnn2d.initialize(input_dim, summary=True, init_type='He', optimizer=SGD, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:32:53.682677Z",
     "start_time": "2019-10-25T18:32:53.674701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer shape=(20, 18, 18), param=1820\n"
     ]
    }
   ],
   "source": [
    "cnn2d.sequential(Conv2d(FN=20,FH=3, FW=3,  pad=0, stride=1))\n",
    "\n",
    "input_dim = (10, 20, 20)\n",
    "cnn2d.initialize(input_dim, summary=True, init_type='He', optimizer=SGD, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】（アドバンス課題）LeNet\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。現在では実用的に使われることはありませんが、歴史的に重要なのは1998年の LeNet です。この構造を再現してMNISTに対して動かし、Accuracyを計算してください。                  \n",
    "![image.png](attachment:image.png)\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n",
    "※上記論文から引用\n",
    "\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。\n",
    "\n",
    "* 畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1\n",
    "* ReLU\n",
    "* 最大プーリング\n",
    "* 畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1\n",
    "* ReLU\n",
    "* 最大プーリング\n",
    "* 平滑化\n",
    "* 全結合層　出力ノード数120\n",
    "* ReLU\n",
    "* 全結合層　出力ノード数84\n",
    "* ReLU\n",
    "* 全結合層　出力ノード数10\n",
    "* ソフトマックス関数\n",
    "# 【問題8】（アドバンス課題）有名な画像認識モデルの調査\n",
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "## 《参考》        \n",
    "https://keras.io/ja/applications/             \n",
    "\n",
    "\n",
    "# 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "\n",
    "7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "高さや幅方向を持たない1×1のフィルタの効果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://qiita.com/ta-ka/items/1c588dd0559d1aad9921"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.444px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
