{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint11 深層学習スクラッチ 畳み込みニューラルネットワーク１\n",
    "* 畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成する\n",
    "* NumPyなど最低限のライブラリのみを使いアルゴリズムを実装する\n",
    "* このSprintでは1次元の **畳み込み層** を作成し、畳み込みの基礎を理解することを目指す\n",
    "* 次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させる\n",
    "* クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:29.748231Z",
     "start_time": "2019-10-25T19:38:22.196830Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# １次元畳み込み層とは\n",
    "* CNNでは画像に対しての2次元畳み込み層が定番だが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装する。\n",
    "* 1次元畳み込みは実用上は自然言語や波形データなどの **系列データ** で使われることが多い\n",
    "* 畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されている\n",
    "## データセットの用意\n",
    "* 検証には引き続きMNISTデータセットを使用する\n",
    "* 1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:31.806949Z",
     "start_time": "2019-10-25T19:38:29.767946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_shape (48000, 784)\n",
      "X_val_shape (12000, 784)\n",
      "y_train_shape (48000,)\n",
      "y_test_shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータのダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "# 型の変換\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# trainデータをtrainとvalに分ける\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print('X_train_shape',X_train.shape)\n",
    "print('X_val_shape',X_val.shape)\n",
    "print('y_train_shape',y_train.shape)\n",
    "print('y_test_shape',y_test.shape)\n",
    "# 仮データ作成\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_mini_train, X_mini_test, y_mini_train, y_mini_test = train_test_split(X_test, y_test, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im２colによる展開\n",
    "フィルターを適用する入力データの場所（３次元のブロック）を横方向１列に展開する                   \n",
    "多くの場合，フィルターの適用領域が重なるため，展開後の要素数は元のブロックの要素数より多くなる（それでもfor文で計算するより高速である）                       \n",
    "im2colで入力データを展開してしまえば，フィルター（重み）を１列に展開して２つの行列の積をとるだけでよくなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "* チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成（チャネル=色の数）\n",
    "* 基本構造は前のSprintで作成した全結合層のFCクラスと同じ\n",
    "* 違いは，データの形状を保持したまま学習する点\n",
    "* 重みの初期化に関するクラスは必要に応じて作り変える\n",
    "* Xavierの初期値などを使う点は全結合層と同様\n",
    "* ここでは **パディング** は考えず、**ストライド**も1に固定する（パディングとストライドで出力サイズが調整できる）\n",
    "#### パディング                            \n",
    "入力データの周囲を０などの固定の値で埋めること．パディングを大きくすると，出力サイズも大きくなる．             \n",
    "→やる理由：出力サイズの調整．(4,4)のデータに(3,3)フィルターをかけると(2,2)になり，畳み込み演算を繰り返すうちに最終的に出力サイズが１になってしまうのを避けるため\n",
    "#### ストライド\n",
    "フィルターを適用する位置の間隔．ストライドを大きくすると出力サイズは小さくなる．\n",
    "* 複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応させる\n",
    "#### チャネル\n",
    "色のことかな？                  \n",
    "#### フィルター\n",
    "* ニューラルネットワークの時でいう「重み」\n",
    "* 入力データとフィルターのチャネル数は同じ値にする\n",
    "* フィルターサイズは好きな値を設定することができる（それが出力のチャネル数になる）\n",
    "* ただし，チャネルごとのフィルターサイズはすべて同じにしなければならない．\n",
    "#### バイアス\n",
    "* CNNのバイアスは常に一つ\n",
    "* その一つの値をフィルター適用後のすべての要素に足す\n",
    "\n",
    "### フォワードプロパゲーション\n",
    "私的フォワードプロパゲーションの解釈               \n",
    "入力された特徴量に対して任意のサイズ（n×m）の重み（=フィルター）を任意のストライドでずらしながら積和し，出力を求めるフェーズ\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x(i+s)+b\n",
    "$$\n",
    "### 更新式\n",
    "$$\n",
    "w'_s=w_s-α\\frac{\\partial L}{\\partial w_s}\\\\\n",
    "b'=b-α\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "### バックプロパゲーション\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_s}=\\sum_{i=0}^{N^{out-1}}\\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b}=\\sum_{i=0}^{N^{out-1}}\\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "### 前の層に流す誤差\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j}=\\sum_{s=0}^{F-1}\\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "$$ただし，j-s<0または，j-s>N_{out}-1のとき，\\frac{\\partial L}{\\partial a_{j-s}}=0です$$                \n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "* 畳み込みを行うと特徴量の数が変化する\n",
    "* パディングやストライドも含めている\n",
    "* 式は以下\n",
    "$$\n",
    "N_{out}=\\frac{N_{in} + 2P - F}{S}+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認する\n",
    "## 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "$$a_i=\\sum_{s=0}^{F-1}x_{(i+s)}w_s+b$$              \n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "$$\\sum_{s=0}^{F-1}x_{(i+s)}w_s$$               \n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:32.750614Z",
     "start_time": "2019-10-25T19:38:32.470364Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "\n",
    "a = np.empty((2, 3))\n",
    "\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "\n",
    "a = a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:33.017669Z",
     "start_time": "2019-10-25T19:38:32.836779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "### 《参考》\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。             \n",
    "https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:33.061550Z",
     "start_time": "2019-10-25T19:38:33.039610Z"
    }
   },
   "outputs": [],
   "source": [
    "# 入力x、重みw、バイアスbを次のようにする\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "#フォワードプロパゲーションをすると出力\n",
    "#a = np.array([35, 50])\n",
    "#loss\n",
    "delta_a = np.array([10, 20])\n",
    "# バックプロパゲーションをすると次のような値\n",
    "#delta_b = np.array([30])\n",
    "#delta_w = np.array([50, 80, 110])\n",
    "#delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:33.116403Z",
     "start_time": "2019-10-25T19:38:33.094464Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"確率的勾配降下法\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        # 更新式\n",
    "        layer.W = layer.W - self.lr*layer.delta_W\n",
    "        layer.B = layer.B- self.lr*layer.delta_B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:34.352289Z",
     "start_time": "2019-10-25T19:38:34.342071Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    def __init__(self,w,b,pad,stride,optimizer,lr):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = w\n",
    "        self.B = b\n",
    "        self.p = pad\n",
    "        self.s = stride\n",
    "        self.f_size = w.size\n",
    "        self.optimizer = optimizer(lr)\n",
    "        \n",
    "        \n",
    "    def n_out_put(self,n):\n",
    "        n_out = int((n + 2*self.p - self.f_size/self.s) + 1)\n",
    "        return n_out\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        n_out = self.n_out_put(x.size)  #出力サイズを計算\n",
    "        a = np.zeros(n_out)             #出力の初期化\n",
    "        for i in range(n_out):\n",
    "            #xのi～フィルタサイズ分までの値にフィルタをかける\n",
    "            a[i] = x[i:i+self.f_size]@self.W + self.B\n",
    "        return a\n",
    "     \n",
    "    def back(self,da):\n",
    "        self.delta_W = np.zeros(self.W.size)\n",
    "        self.delta_B = np.sum(da)\n",
    "        for i in range(self.W.size):\n",
    "            self.delta_W[i] = x[i:i + da.size]@da\n",
    "        \n",
    "        delta_x = []\n",
    "        for j in range(len(x)):\n",
    "            temp = 0\n",
    "            for s in range(len(self.W)):\n",
    "                if (j-s >= 0) and (j-s < len(da)):\n",
    "                    temp += da[j-s]*self.W[s]\n",
    "            delta_x.append(temp)\n",
    "            \n",
    "        self.optimizer.update(self)\n",
    "        return delta_x\n",
    "    \n",
    "    def fit(self,x,da):\n",
    "        n = x.size\n",
    "        n_out = self.n_out_put(n)\n",
    "        \n",
    "        self.a = self.forward(x)\n",
    "        self.delta_x = self.back(da)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:37.006038Z",
     "start_time": "2019-10-25T19:38:34.664805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_x [30, 110, 170, 140]\n",
      "delta_w [ 50.  80. 110.]\n",
      "delta_b 30\n",
      "更新後のw [2.5 4.2 5.9]\n",
      "更新後のb [0.7]\n"
     ]
    }
   ],
   "source": [
    "# 初期化，学習，確認\n",
    "conv1 = SimpleConv1d(w=w,b=b,pad=0,stride=1,optimizer=SGD,lr=0.01)\n",
    "conv1.fit(x,delta_a)\n",
    "print('delta_x',conv1.delta_x)\n",
    "print('delta_w',conv1.delta_W)\n",
    "print('delta_b',conv1.delta_B)\n",
    "print('更新後のw',conv1.W)\n",
    "print('更新後のb',conv1.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。                     \n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "### 《補足》\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。**(バッチサイズ、チャンネル数、特徴量数)**または**(バッチサイズ、特徴量数、チャンネル数)**が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、**(チャンネル数、特徴量数)**です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:39.046485Z",
     "start_time": "2019-10-25T19:38:39.041499Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "#a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
    "delta_a = np.array([[52,56], [32,35], [9,11]])\n",
    "#FN,FC,FW = w.shape\n",
    "C,W = x.shape\n",
    "stride = 1\n",
    "pad = 0\n",
    "#da_N,da_W = delta_a.shape\n",
    "#da_N,da_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:39.429675Z",
     "start_time": "2019-10-25T19:38:39.421724Z"
    }
   },
   "outputs": [],
   "source": [
    "class OUTPUT:\n",
    "    \"\"\"一次減畳み込みの出力サイズ\"\"\"\n",
    "    def __init__(self, n,pad,f_size,strider):\n",
    "        self.n = n #入力サイズ\n",
    "        self.p = pad #パディングサイズ\n",
    "        self.f_size = f_size #フィルタサイズ\n",
    "        self.s = stride #ストライド\n",
    "        \n",
    "    def n_out_put(self):\n",
    "        n_out = int((self.n + 2*self.p - self.f_size/self.s) + 1)\n",
    "        return n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:40.081338Z",
     "start_time": "2019-10-25T19:38:40.067343Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, w, b , optimizer,stride, pad):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = w\n",
    "        self.B = b\n",
    "        self.s = stride\n",
    "        self.p = pad\n",
    "        self.FN, self.FC, self.FW = self.W.shape\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーション\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.A = X\n",
    "        feature_num = self.A.shape[1]\n",
    "        \n",
    "        a = np.zeros([self.FN, feature_num - 2])\n",
    "        for output in range(self.FN):\n",
    "            for j in range(self.FW - 1):\n",
    "                sig = 0\n",
    "                for chanel in range(self.FC):\n",
    "                    for i in range(self.FW):\n",
    "                        sig += X[chanel, i+j] * self.W[output, chanel, j]\n",
    "                a[output, j] = sig + self.B[output]\n",
    "        \n",
    "        return a\n",
    "\n",
    "    \n",
    "    def _back(self, dA):\n",
    "        \"\"\"\n",
    "        バックプロパゲーション\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        output = OUTPUT(self.A.shape[0],self.p,self.FW,self.s)\n",
    "        self.n_out = output.n_out_put()\n",
    "        self.delta_B = np.sum(dA, axis=1)\n",
    "        feature_num = self.A.shape[1]\n",
    "        \n",
    "        #delta_Wの計算\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "        for output in range(self.FN):    \n",
    "            for chanel in range(self.FC):    \n",
    "                for i in range(self.FW):\n",
    "                    for j in range(self.FW -1):\n",
    "                        self.delta_W[output, chanel, i] += dA[output, j]*self.A[chanel, j+i]\n",
    "                    \n",
    "        \n",
    "        #dZの計算\n",
    "        dZ = np.zeros_like(self.A)\n",
    "        for output in range(self.FN):\n",
    "            for chanel in range(self.FC):\n",
    "                for j in range(feature_num):\n",
    "                    sigma=0\n",
    "                    for s in range(self.FW):\n",
    "                        if j - s < 0 or j - s > self.n_out -1:\n",
    "                            pass\n",
    "                        else:\n",
    "                            sigma += dA[output,  j-s] * self.W[output, chanel, s]\n",
    "                            \n",
    "                    dZ[chanel, j] += sigma\n",
    "        \n",
    "        # 更新\n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:40.619557Z",
     "start_time": "2019-10-25T19:38:40.606551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = Conv1d(w, b, SGD(0.01), stride, pad)\n",
    "print(cnn._forward(x))\n",
    "cnn._back(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証\n",
    "# 【問題8】学習と推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 **平滑化** を行なってください。\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。\n",
    "## 重みの初期化クラス\n",
    "今回は，活性化関数にReLUを使用するため，Heとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:48.472892Z",
     "start_time": "2019-10-25T19:38:48.467905Z"
    }
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    ReLUの時の重みの初期値\n",
    "    \"\"\"\n",
    "    def W(self, FN, FC, FW):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        FN : int アウトプットサイズ\n",
    "        FC : int チャネル数\n",
    "        FW : int フィルタサイズ\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2/FN)\n",
    "        W = self.sigma * np.random.randn((FN,FC,FW))\n",
    "        return W\n",
    "    \n",
    "    def B(self, FN):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        FN : int アウトプットサイズ\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(FN)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適化手法クラス\n",
    "今回は確率的勾配降下法とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:49.704648Z",
     "start_time": "2019-10-25T19:38:49.699629Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr*layer.dW\n",
    "        layer.B = layer.B - self.lr*layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数\n",
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:51.041763Z",
     "start_time": "2019-10-25T19:38:51.037807Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0,A)\n",
    "        return Z \n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        #np.signでAを-1，0，1に変換\n",
    "        dA = dZ * (np.maximum(0,np.sign(self.A)))\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:52.209862Z",
     "start_time": "2019-10-25T19:38:52.204899Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,A3):\n",
    "        exp_A = np.exp(A3)\n",
    "        sum_A = np.sum(exp_A,axis=1).reshape(-1,1)\n",
    "        Z3 = exp_A/sum_A\n",
    "        return Z3\n",
    "    \n",
    "    def backward(self,Z3,Y):\n",
    "        batch_n = Y.shape[0]\n",
    "        # A3の勾配\n",
    "        dA3 = Z3 - Y\n",
    "        #print('soft_max_dA3',dA3.shape)\n",
    "        \n",
    "        # 交差エントロピー誤差\n",
    "        loss = -np.sum(Y*np.log(Z3+1e-7)) / batch_n\n",
    "             \n",
    "        return dA3,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力サイズを求める関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:57.886623Z",
     "start_time": "2019-10-25T19:38:57.883623Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_size(n_in,pad,FW,stride):\n",
    "    \"\"\"\n",
    "    一次元の出力サイズを求める関数\n",
    "    Parameters\n",
    "    -----------------\n",
    "    n_in   : int　特徴量の数\n",
    "    pad : int パディングの数\n",
    "    FW  : int フィルタのサイズ\n",
    "    stride   : int ストライドのサイズ  \n",
    "    Return\n",
    "    ------------------\n",
    "    n_out : 出力の特徴量数\n",
    "    \"\"\"\n",
    "    n_out = int((n_in + 2*pad - FW/stride) + 1)\n",
    "    return n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畳み込み層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:38:59.194706Z",
     "start_time": "2019-10-25T19:38:59.180744Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer:最適化手法\n",
    "    initializer:重みとバイアスの初期化方法\n",
    "    FN : int アウトプットサイズ\n",
    "    FC : int チャネル数（入力のチャネルと同じ）\n",
    "    FW : int フィルタサイズ\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer,initializer, FN, FC, FW, pad, stride):\n",
    "        self.optimizer = optimizer\n",
    "        self.FN = FN\n",
    "        self.FC = FC\n",
    "        self.FW = FW\n",
    "        self.pad = pad\n",
    "        self.s = stride\n",
    "        self.W = initializer.W(FN, FC, FW)\n",
    "        self.B = initializer.B(FN)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーション\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (input_chanel,feature_num)\n",
    "        Return\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (FN,OW)\n",
    "        \"\"\"\n",
    "        self.Z = X\n",
    "        self.feature_num = X.shape[1]\n",
    "        \n",
    "        A = np.zeros([self.FN, feature_num - 2])\n",
    "        \n",
    "        for output in range(self.FN):\n",
    "            for j in range(self.FW - 1):\n",
    "                sig = 0\n",
    "                for chanel in range(self.FC):\n",
    "                    for i in range(self.FW):\n",
    "                        sig += X[chanel, i+j] * self.W[output, chanel, j]\n",
    "                A[output, j] = sig + self.B[output]\n",
    "        \n",
    "        return A\n",
    "\n",
    "    \n",
    "    def _back(self, dA):\n",
    "        \"\"\"\n",
    "        バックプロパゲーション\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        n_out = output_size(self.feature_num, self.pad, self.FW, self.s)\n",
    "        self.delta_B = np.sum(dA, axis=1)\n",
    "        feature_num = self.A.shape[1]\n",
    "        \n",
    "        #delta_Wの計算\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        for output in range(self.FN):    \n",
    "            for chanel in range(self.FC):    \n",
    "                for i in range(self.FW):\n",
    "                    for j in range(self.FW -1):\n",
    "                        self.dW[output, chanel, i] += dA[output, j]*self.Z[chanel, j+i]\n",
    "                    \n",
    "        \n",
    "        #dZの計算\n",
    "        dZ = np.zeros_like(self.Z)\n",
    "        for output in range(self.FN):\n",
    "            for chanel in range(self.FC):\n",
    "                for j in range(self.feature_num):\n",
    "                    sigma=0\n",
    "                    for s in range(self.FW):\n",
    "                        if j - s < 0 or j - s > n_out -1:\n",
    "                            pass\n",
    "                        else:\n",
    "                            sigma += dA[output,  j-s] * self.W[output, chanel, s]\n",
    "                            \n",
    "                    dZ[chanel, j] += sigma\n",
    "        \n",
    "        # 更新\n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:39:05.456379Z",
     "start_time": "2019-10-25T19:39:05.449361Z"
    }
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス（ガウス分布とか）\n",
    "    optimizer : 最適化手法のインスタンス（確率的勾配降下とか）\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1,n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力（最初はX,以降はZ）\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.Z = X\n",
    "        A = np.dot(self.Z,self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = dA@self.W.T\n",
    "        self.dW = self.Z.T@dA\n",
    "        self.dB = np.sum(dA,axis=0)\n",
    "        \n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T19:58:52.066431Z",
     "start_time": "2019-10-25T19:58:52.023529Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scratch1dCNN:\n",
    "    def __init__(self, lr, epochs, FN,FW,initializer, optimizer, activation, pad, stride):\n",
    "        #self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.initializer = initializer()     #重み・バイアスの初期化方法\n",
    "        self.optimizer = optimizer(lr)       #最適化手法のインスタンス\n",
    "        self.activation = activation         # 活性化関数\n",
    "        self.FN = FN\n",
    "        self.FW = FW\n",
    "        self.pad = pad\n",
    "        self.s = stride\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        # lossを入れる箱\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.C, self.feature_num = X.shape[0]\n",
    "        y_out = np.unique(y).size\n",
    "        x_out = output_size(self.feature_num,self.pad, self.FW,self.s)\n",
    "        \n",
    "        # 正解データをOne-Hot表現に\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        # 畳み込み層\n",
    "        self.conv = Conv1d(self.optimizer, self.initializer, self.FN, self.FC, self.FW, self.pad, self.s)\n",
    "        #活性化層\n",
    "        self.activation1 = self.activation()\n",
    "        #全結合層\n",
    "        fc = FC(x_out, y_out, self.initializer, self.optimizer)\n",
    "        self.activation2 = Softmax()\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            #lossの初期化\n",
    "            loss = 0\n",
    "            # ミニバッチの取得\n",
    "            #get_mini_batch = GetMiniBatch(X, y_one_hot, batch_size=self.batch_n)\n",
    "            # ミニバッチ開始\n",
    "            #for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            # フォワードプロパゲーション\n",
    "            A1 = self.conv._forward(X)\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.fc.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            #A3 = self.FC3.forward(Z2)\n",
    "            #Z3 = self.activation3.forward(A3)\n",
    "\n",
    "            #softmax & 交差エントロピー\n",
    "            dA2,loss = self.activation2.backward(Z2, mini_y_train)\n",
    "            loss += loss\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            dZ2 = self.fc.backward(dA2)\n",
    "            dA1 = self.activation2.backward(dZ2)\n",
    "            dZ0 = self.conv._back(dA1)\n",
    "                        \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose is True:\n",
    "                print('Train Data Loss epoch {0} : {1}'.format(e,self.loss[-1]))\n",
    "                print('Validation Loss epoch {0} : {1}'.format(e,self.val_loss[-1]))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : 次の形のndarray, shape (n_samples, 1)  推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.conv._forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.fc.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        pred = np.argmax(Z2,axis=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T20:00:52.308202Z",
     "start_time": "2019-10-25T20:00:52.304213Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializer=初期化(Simple,He,Xa)，optimizer=最適化手法(SGD or AdaGrad)，activation=活性化関数(Sigmoid,Tanh,ReLU)\n",
    "cnn = Scratch1dCNN(lr=0.01, epochs=5, \n",
    "                    FN=3,FW=3,initializer=HeInitializer, optimizer=SGD, activation=ReLU, pad=0, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T20:01:29.420477Z",
     "start_time": "2019-10-25T20:01:28.623319Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4f3433b64f48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mini_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_mini_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-56acee50b5fc>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0my_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "cnn.fit(X_mini_train,y_mini_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[164., 272., 380.],\n",
       "        [272., 380., 488.]],\n",
       "\n",
       "       [[102., 169., 236.],\n",
       "        [169., 236., 303.]],\n",
       "\n",
       "       [[ 31.,  51.,  71.],\n",
       "        [ 51.,  71.,  91.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#バックプロパゲーションのdelta_Wを求めるフェーズ\n",
    "delt_w = np.zeros(FN*FC*FW).reshape(FN,FC,FW)\n",
    "#delta_aのチャネル数分分けて計算するイメージ\n",
    "for dac in range(da_N):\n",
    "    #出来上がるdelta_w分計算するイメージ\n",
    "    for fw in range(FW):\n",
    "        #dwのFN番目の全部のチャネルの横方向番目を計算していく\n",
    "        delt_w[dac,:,fw] = np.sum(delta_a[dac,:]*x[:,fw:fw+da_W],axis=1)\n",
    "        #delt_w[fn,:,fw] = np.sum(delta_a[fn,:]*x[:,fw:fw+da_W])\n",
    "delt_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
