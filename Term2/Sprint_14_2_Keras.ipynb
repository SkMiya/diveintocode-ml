{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint ディープラーニングフレームワーク2\n",
    "# １．公式Example\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。\n",
    "\n",
    "# 【問題1】公式Exampleを分担して実行\n",
    "TensorFLowの公式Exampleを分担して実行してください。                     \n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "### research\n",
    "定番のモデルから最新のモデルまで多様なコードが公開されています。                           \n",
    "https://github.com/tensorflow/models/tree/master/research                             \n",
    "### tutorials\n",
    "TensorFLowのチュートリアルとして用意された簡単なモデルが含まれています。                      \n",
    "https://github.com/tensorflow/models/tree/master/tutorials                    \n",
    "\n",
    "## 事前学習済みのConvNetを使用した転移学習\n",
    "このチュートリアルでは、事前に訓練されたネットワークからの転移学習を使用して、猫と犬の画像を分類する方法について説明します。これにより、ネットワークをゼロからトレーニングすることで、見たよりも高い精度を得ることができます。\n",
    "\n",
    "**事前訓練されたモデル**は、以前に、典型的には大規模な画像分類タスクに、大きなデータセットで訓練された保存されたネットワークです。事前学習済みのモデルをそのまま使用するか、事前学習済みの修道院を使用して学習を移行できます。**転移学習**の背後にある直観は、このモデルが大きく一般的な十分なデータセットでトレーニングされた場合、このモデルは視覚世界の一般的なモデルとして効果的に機能するということです。タスクに固有の独自のモデルの基礎としてこれらのモデルを使用することにより、大きなデータセットで大きなモデルをトレーニングする必要なく、これらの学習された機能マップを活用できます。                    \n",
    "                     \n",
    "事前学習済みモデルを使用した転移学習には2つのシナリオがあります。\n",
    "\n",
    "### 1.特徴抽出 \n",
    "-以前のネットワークで学習した表現を使用して、新しいサンプルから意味のある特徴を抽出します。事前学習済みモデルの上に、ゼロから学習する新しい分類子を追加するだけで、データセット用に以前に学習した特徴マップを再利用できます。事前学習済みモデル全体を使用しますか、それとも畳み込みベースのみを使用しますか？-これらの事前トレーニングされたconvnet（畳み込みベース）の特徴抽出部分を使用します。これは、これらが一般的な特徴であり、画像上の概念を学習する可能性が高いためです。ただし、事前学習済みモデルの分類部分は、多くの場合、元の分類タスクに固有であり、その後、モデルが学習されたクラスのセットに固有です。\n",
    "### 2.Fine Tuning\n",
    "-特徴抽出に使用される凍結モデルベースの最上層のいくつかの凍結を解除し、新しく追加された分類レイヤーと凍結モデルの最後のレイヤーの両方を共同でトレーニングします。これにより、最終的な分類子に加えて高次のフィーチャ表現を「微調整」して、関連する特定のタスクにより関連するようにすることができます。                                     \n",
    "一般的な機械学習のワークフローに従います。                                  \n",
    "1.データを調べて理解する                             \n",
    "2.入力パイプラインを構築する-画像分類チュートリアルで行ったようにKeras ImageDataGeneratorを使用します                                \n",
    "3.モデルを作成する                            \n",
    "* 事前学習済みのモデル（および事前学習済みの重み）を読み込む\n",
    "* 上に分類レイヤーを積み重ねる\n",
    "4.モデルを訓練する                            \n",
    "5.モデルを評価する                        \n",
    "                       \n",
    "事前にトレーニングされたconvnetを特徴抽出として使用し、ベースモデルの最後の数層をトレーニングするために微調整する例を見ていきます。\n",
    "\n",
    "\n",
    "## 転移学習とは（自分なりの解釈）\n",
    "* 転移学習とは、すでに学習したモデルを別の領域に適応させる技術のこと      \n",
    "* 何も知らない赤ちゃん状態から学習するより，その領域のことはあんま知らなくても，ある程度教育受けてる大人の方が学習するのは早いよね，って話．\n",
    "* ImageNetという大規模なデータセット学習した1000種類の画像を分類できるモデルを利用して，イヌorネコを分類する            \n",
    "                                          \n",
    "### 転移学習には大きく分けて二つのやり方がある。                   \n",
    "１． 訓練済みモデルの出力層に近い側のパラメータのみ訓練し、上流のパラメータは固定する。                  \n",
    "     →再訓練に使えるデータセットの規模が非常に小さい場合や、訓練時間を短縮したい場合に用いる。（今回コッチ）            \n",
    "２．訓練済みモデルの全てのパラメータを再訓練する。                          \n",
    "→**ファインチューニング**と呼び、通常はこちらの方が高い精度が得られやすい。\n",
    "### 転移学習のプロセス\n",
    "* 第一段：入力画像から、特徴量(ボトルネック特徴量)を抽出する\n",
    "* 第二段：ボトルネック特徴量を用いて、クラス分類をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is  1.14.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"TensorFlow version is \", tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのダウンロード-cats_and_dogs_filtered.zip                         \n",
    "KaggleのDogs vs Catsデータセットのフィルターバージョンをダウンロードします。次に、ダウンロードしたzipファイルを「/ tmp /」ディレクトリに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = tf.keras.utils.get_file(origin=\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n",
    "                                   fname=\"cats_and_dogs_filtered.zip\", extract=True)\n",
    "base_dir, _ = os.path.splitext(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 猫と犬のデータセットのトレーニングと検証を準備する\n",
    "猫のデータセットと犬のデータセットのトレーニングおよび検証ディレクトリを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training cat images: 1000\n",
      "Total training dog images: 1000\n",
      "Total validation cat images: 500\n",
      "Total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "#トレーニング猫の写真のディレクトリ\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "print ('Total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "\n",
    "# トレーニング犬の写真のディレクトリ\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "print ('Total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "#検証猫の写真のあるディレクトリ\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "print ('Total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "\n",
    "# 検証犬の写真のあるディレクトリ\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "print ('Total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像増強を使用した画像データジェネレーターの作成\n",
    "ImageDataGeneratorを使用して画像のスケールを変更します。                \n",
    "keras.preprocessing.image.ImageDataGenerator http://pynote.hatenablog.com/entry/keras-image-data-generator                            \n",
    "トレインジェネレーターを作成するには、トレインデータセットディレクトリ、イメージサイズ、バッチサイズ、バイナリ分類モードの場所を指定します。                      \n",
    "検証ジェネレーターも同じ方法で作成されます\n",
    "#### ジェネレータってなに．\n",
    "プログラムにおいて、数列の各要素の値などを次々と生成（ジェネレート）し他の手続きに渡す、という機能を持っている手続き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160 # すべての画像は160x160にサイズ変更されます\n",
    "batch_size = 32\n",
    "\n",
    "#引数 rescale に指定した値で、各変換を行う前に画素値を rescale 倍する。\n",
    "#[0, 255] で表される画素値を [0, 1] に正規化する場合などに使用する。\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1./255)\n",
    "\n",
    "validation_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#'flow_from_directory'がデータのあるディレクトリからデータを自動で読み取ってジェネレータを作ってくれる\n",
    "#train_datagenジェネレーターを使用した20バッチのフロートレーニングイメージ\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                train_dir,  # トレーニング画像のソースディレクトリ\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                #binary_crossentropy損失を使用するため、バイナリラベルが必要です\n",
    "                class_mode='binary')\n",
    "\n",
    "# test_datagenジェネレーターを使用した20バッチのフロー検証画像\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                validation_dir, # 検証イメージのソースディレクトリ\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前訓練されたconvnetから基本モデルを作成する\n",
    "**MobileNet V2モデル**                       \n",
    "ImageNetデータセット（1.4M画像の大規模なデータセット）を，1000クラスに分類するモデル                                 \n",
    "                                   \n",
    "                                  \n",
    "#### 基本モデルの作成方法                         \n",
    "まず、特徴抽出に使用するMobileNet V2の中間層を選択する必要があります。一般的な方法は、平坦化操作の前の最後の層、いわゆる「ボトルネック層」（＝全結合層の一個前の層＝畳み込み層の最後の層）の出力を使用することです。ここでの理由は、次の完全に接続されたレイヤーは、ネットワークがトレーニングされたタスクに特化しすぎているため、これらのレイヤーが学習した機能は新しいタスクにはあまり役に立たないからです。ただし、ボトルネック機能には一般性が多くあります。             \n",
    "                  \n",
    "(自分的解釈)                 \n",
    "全結合層は1000種類の画像を分類するレイヤーなのでイラナイ．重みを学習する層はいる．                       \n",
    "                                \n",
    "ImageNetでトレーニングされた重みでプリロードされたMobileNet V2モデルをインスタンス化します。include_top = False引数を指定することにより、最上位に分類レイヤーを含まないネットワークをロードします。これは、特徴抽出に最適です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "\n",
    "# 事前トレーニング済みモデルMobileNet V2から基本モデルを作成します\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,#MobileNetV2モデルから全結合層を除く\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴抽出\n",
    "前の手順で作成した畳み込みベースをフリーズし、それを特徴抽出として使用し、その上に分類子を追加して、最上位の分類子をトレーニングします。\n",
    "\n",
    "### 畳み込みベースをフリーズする\n",
    "モデルをコンパイルしてトレーニングする前に、畳み込みベースをフリーズすることが重要です。フリーズ（または設定layer.trainable = False）することにより、これらのレイヤーの重みがトレーニング中に更新されないようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_160\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 161, 161, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 80, 80, 32)   864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 80, 80, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 80, 80, 32)   0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 80, 80, 32)   288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 80, 80, 32)   128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 80, 80, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 80, 80, 16)   512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 80, 80, 16)   64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 80, 80, 96)   1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 80, 80, 96)   384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 80, 80, 96)   0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 81, 81, 96)   0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 40, 40, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 40, 40, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 40, 40, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 40, 40, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 40, 40, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 40, 40, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 40, 40, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 40, 40, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 40, 40, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 40, 40, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 40, 40, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 40, 40, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 40, 40, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 41, 41, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 20, 20, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 20, 20, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 20, 20, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 20, 20, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 20, 20, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 20, 20, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 20, 20, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 20, 20, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 20, 20, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 20, 20, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 20, 20, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 20, 20, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 20, 20, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 20, 20, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 21, 21, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 10, 10, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 10, 10, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 10, 10, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 10, 10, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 10, 10, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 10, 10, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 10, 10, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 10, 10, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 10, 10, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 10, 10, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 10, 10, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 10, 10, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 10, 10, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 10, 10, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 10, 10, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 10, 10, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 10, 10, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 10, 10, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 10, 10, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 10, 10, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 10, 10, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 10, 10, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 10, 10, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 10, 10, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 10, 10, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 10, 10, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 10, 10, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 10, 10, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 10, 10, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 10, 10, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 10, 10, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 10, 10, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 10, 10, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 11, 11, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 5, 5, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 5, 5, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 5, 5, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 5, 5, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 5, 5, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 5, 5, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 5, 5, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 5, 5, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 5, 5, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 5, 5, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 5, 5, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 5, 5, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 5, 5, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 5, 5, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 5, 5, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 5, 5, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 5, 5, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 5, 5, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 5, 5, 1280)   0           Conv_1_bn[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#畳み込み層のフリーズ（学習せずにそのまま使う）\n",
    "base_model.trainable = False\n",
    "#基本モデルのアーキテクチャを見てみましょう\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類ヘッドを追加する\n",
    "次に、基本モデルの上にいくつかのレイヤーを追加しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,# MobileNet V2 から全結合層を除いたもの\n",
    "  keras.layers.GlobalAveragePooling2D(),#プーリング層\n",
    "  keras.layers.Dense(1, activation='sigmoid')#全結合層\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルをコンパイルする\n",
    "モデルをトレーニングする前にコンパイルする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの1.2Kのトレーニング可能なパラメーターは、2つのTensorFlow Variableオブジェクト、2つの密なレイヤーの重みとバイアスに分割されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルを訓練する\n",
    "10エポックのトレーニングの後、約94％の精度を得ることができます。\n",
    "\n",
    "時間があれば、収束するまでトレーニングします（50エポック、96％の精度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 259s 4s/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1436 - val_acc: 0.9637\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 202s 3s/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.1302 - val_acc: 0.9718\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 174s 3s/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.1380 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 193s 3s/step - loss: 0.0044 - acc: 0.9980 - val_loss: 0.1365 - val_acc: 0.9718\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 184s 3s/step - loss: 0.0051 - acc: 0.9975 - val_loss: 0.1329 - val_acc: 0.9698\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 193s 3s/step - loss: 0.0102 - acc: 0.9980 - val_loss: 0.1378 - val_acc: 0.9708\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 194s 3s/step - loss: 0.0145 - acc: 0.9959 - val_loss: 0.1444 - val_acc: 0.9688\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 185s 3s/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1426 - val_acc: 0.9688\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 182s 3s/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1334 - val_acc: 0.9708\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 183s 3s/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1386 - val_acc: 0.9708\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              workers=4,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習曲線\n",
    "MobileNet V2ベースモデルを固定機能抽出ツールとして使用する場合の、トレーニングと検証の精度/損失の学習曲線を見てみましょう。        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファインチューニング\n",
    "特徴抽出実験では、MobileNet V2ベースモデルの上にいくつかのレイヤーのみをトレーニングしていました。事前トレーニング済みネットワークの重みは、トレーニング中に更新されませんでした。パフォーマンスをさらに向上させる1つの方法は、トップレベルの分類器のトレーニングと一緒に、事前トレーニング済みモデルのトップレイヤーの重みを「微調整」することです。トレーニングプロセスにより、一般的な機能マップから、特にデータセットに関連付けられている機能に重みが調整されます。                         \n",
    "                               \n",
    "                               \n",
    "注：これは、事前トレーニング済みモデルをトレーニング不可に設定して最上位分類器をトレーニングした後にのみ試行する必要があります。事前に訓練されたモデルの上にランダムに初期化された分類器を追加し、すべてのレイヤーを共同で訓練しようとすると、勾配更新の大きさが大きくなり（分類器からのランダムな重みのため）、事前に訓練されたモデルが学んだことをすべて忘れてください。                       \n",
    "                                  \n",
    "                                  \n",
    "さらに、事前学習済みモデルのすべてのレイヤーではなく、事前学習済みモデルの最上位レイヤーを微調整する理由は次のとおりです。convnetでは、レイヤーが上位になるほど、より特殊化されます。convnetの最初の数層は、ほとんどすべての種類の画像に一般化される非常にシンプルで一般的な機能を学びました。しかし、上に行くほど、モデルがトレーニングされたデータセットに固有の機能が増えています。微調整の目標は、これらの特殊な機能を新しいデータセットで動作するように適合させることです。\n",
    "\n",
    "### モデルの最上層の固定を解除します\n",
    "必要なのはbase_model、をフリーズ解除し、最下層をトレーニング不能に設定することだけです。次に、モデルを再コンパイルし（これらの変更を有効にするために必要）、トレーニングを再開します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  155\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True  #FC層から下流も使う\n",
    "#基本モデルにいくつのレイヤーがあるか見てみましょう\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "#このレイヤー以降をFine tuning\n",
    "fine_tune_at = 100\n",
    "\n",
    "#`fine_tune_at`レイヤーの前のすべてのレイヤーをフリーズします\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルをコンパイルする\n",
    "はるかに低いトレーニングレートを使用してモデルをコンパイルします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 1,863,873\n",
      "Non-trainable params: 395,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルのトレーニングを続ける\n",
    "先に収束するようにトレーニングした場合、これにより、数パーセントの精度が向上します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 286s 5s/step - loss: 0.1595 - acc: 0.9487 - val_loss: 0.0940 - val_acc: 0.9688\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 271s 4s/step - loss: 0.0861 - acc: 0.9746 - val_loss: 0.0830 - val_acc: 0.9728\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 280s 5s/step - loss: 0.0677 - acc: 0.9802 - val_loss: 0.0784 - val_acc: 0.9768\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 284s 5s/step - loss: 0.0427 - acc: 0.9914 - val_loss: 0.0815 - val_acc: 0.9718\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 266s 4s/step - loss: 0.0400 - acc: 0.9903 - val_loss: 0.0935 - val_acc: 0.9677\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 271s 4s/step - loss: 0.0243 - acc: 0.9944 - val_loss: 0.0955 - val_acc: 0.9677\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 270s 4s/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.1033 - val_acc: 0.9667\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 272s 4s/step - loss: 0.0078 - acc: 0.9995 - val_loss: 0.1196 - val_acc: 0.9657\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 195s 3s/step - loss: 0.0074 - acc: 0.9985 - val_loss: 0.1223 - val_acc: 0.9667\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 189s 3s/step - loss: 0.0058 - acc: 0.9995 - val_loss: 0.1429 - val_acc: 0.9617\n"
     ]
    }
   ],
   "source": [
    "history_fine = model.fit_generator(train_generator,\n",
    "                                   steps_per_epoch = steps_per_epoch,\n",
    "                                   epochs=epochs,\n",
    "                                   workers=4,\n",
    "                                   validation_data=validation_generator,\n",
    "                                   validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習曲線\n",
    "MobileNet V2ベースモデルの最後の数層とその上の分類子を微調整するときの、トレーニングと検証の精度/損失の学習曲線を見てみましょう。検証の損失はトレーニングの損失よりもはるかに大きいことに注意してください。\n",
    "\n",
    "注：トレーニングデータセットはかなり小さく、MobileNet V2がトレーニングされた元のデータセットに似ているため、微調整を行うとオーバーフィットする可能性があります。\n",
    "\n",
    "収束（epochs=50）にトレーニングすると、結果のグラフは次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['acc']\n",
    "val_acc += history_fine.history['val_acc']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.9, 1])\n",
    "plt.plot([epochs-1,epochs-1], plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 0.2])\n",
    "plt.plot([epochs-1,epochs-1], plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要なポイント\n",
    "要約すると、精度を向上させるために事前トレーニング済みモデルを使用して転送学習を行う方法に関するこのチュートリアルでカバーした内容です：* 特徴抽出に事前トレーニング済みモデルを使用-小さなデータセットを使用する場合、一般的に同じドメイン内のより大きなデータセットでトレーニングされたモデルによって学習された機能。これは、事前にトレーニングされたモデルをインスタンス化し、完全に接続された分類器を上に追加することにより行われます。事前に訓練されたモデルは「凍結」されており、分類器の重みのみが訓練中に更新されます。この場合、畳み込みベースは各画像に関連付けられているすべての特徴を抽出し、これらの特徴のセットがどのクラスに属しているかを判断する分類器をトレーニングします。* 微調整事前トレーニング済みモデル-パフォーマンスをさらに向上させるために、事前調整済みモデルの最上位レイヤーを新しいデータセットに再調整して再利用することができます。この場合、データセット固有の高度に指定された高レベルの特徴を学習するように重みを調整します。これは、トレーニングデータセットが大きく、事前トレーニングモデルがトレーニングされた元のデータセットと非常によく似ている場合にのみ意味があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ２．異なるフレームワークへの書き換え\n",
    "Sprint14で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。                \n",
    "* Iris（Iris-versicolorとIris-virginicaのみの2値分類）             \n",
    "* Iris（3種類全ての目的変数を使用して多値分類）\n",
    "* House Prices\n",
    "* MNIST\n",
    "### Kerasへの書き換え\n",
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。                                   \n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題２】Irisの２値分類\n",
    "#### ２値分類の出力層の活性化関数の違い\n",
    "* ２値分類One-Hot表現にしていない場合・・・sigmoidを通して，0.5以上を１未満を0とする\n",
    "* ２値分類をOne-Hot表現にした場合・・・softmaxを通して，一番大きい確率のラベルに振り分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (64, 4)\n",
      "y_shape (64, 1)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv('Iris.csv')\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print('X_shape:',X_train.shape)\n",
    "print('y_shape',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.6960 - acc: 0.5469 - val_loss: 0.7073 - val_acc: 0.3750\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 311us/step - loss: 0.6906 - acc: 0.5313 - val_loss: 0.6849 - val_acc: 0.3750\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.6395 - acc: 0.7188 - val_loss: 0.6005 - val_acc: 0.7500\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.6316 - acc: 0.7500 - val_loss: 0.6170 - val_acc: 0.8750\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 280us/step - loss: 0.6064 - acc: 0.8906 - val_loss: 0.5811 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.5900 - acc: 0.8750 - val_loss: 0.5630 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 299us/step - loss: 0.5835 - acc: 0.9219 - val_loss: 0.5528 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.5818 - acc: 0.7969 - val_loss: 0.5406 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 281us/step - loss: 0.5427 - acc: 0.8281 - val_loss: 0.5245 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.5229 - acc: 0.9531 - val_loss: 0.4753 - val_acc: 0.9375\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.5051 - acc: 0.9219 - val_loss: 0.4663 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.4885 - acc: 0.9531 - val_loss: 0.4376 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.4684 - acc: 0.9531 - val_loss: 0.4249 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.4452 - acc: 0.9531 - val_loss: 0.4036 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 250us/step - loss: 0.4252 - acc: 0.9531 - val_loss: 0.3663 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 297us/step - loss: 0.4056 - acc: 0.9844 - val_loss: 0.3362 - val_acc: 0.9375\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.3908 - acc: 1.0000 - val_loss: 0.3285 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.3686 - acc: 0.9688 - val_loss: 0.2952 - val_acc: 0.9375\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 296us/step - loss: 0.3606 - acc: 0.9531 - val_loss: 0.2934 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 202us/step - loss: 0.3286 - acc: 0.9688 - val_loss: 0.2598 - val_acc: 0.9375\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.3459 - acc: 0.9531 - val_loss: 0.2859 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 312us/step - loss: 0.3139 - acc: 0.9688 - val_loss: 0.2551 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 202us/step - loss: 0.3011 - acc: 0.9531 - val_loss: 0.2203 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 312us/step - loss: 0.2756 - acc: 0.9844 - val_loss: 0.2154 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.2613 - acc: 0.9688 - val_loss: 0.2004 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 202us/step - loss: 0.2511 - acc: 0.9688 - val_loss: 0.2110 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.2379 - acc: 0.9688 - val_loss: 0.1773 - val_acc: 0.9375\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.2449 - acc: 0.9219 - val_loss: 0.2166 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.2347 - acc: 0.9844 - val_loss: 0.1666 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 281us/step - loss: 0.2203 - acc: 0.9375 - val_loss: 0.1824 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.2102 - acc: 0.9844 - val_loss: 0.1476 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 280us/step - loss: 0.1976 - acc: 0.9531 - val_loss: 0.1449 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.2045 - acc: 0.9531 - val_loss: 0.1328 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.1908 - acc: 0.9531 - val_loss: 0.1352 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.1943 - acc: 0.9531 - val_loss: 0.1328 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 171us/step - loss: 0.1756 - acc: 0.9688 - val_loss: 0.1204 - val_acc: 0.9375\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 281us/step - loss: 0.1776 - acc: 0.9531 - val_loss: 0.1223 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1742 - acc: 0.9531 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.1640 - acc: 0.9531 - val_loss: 0.1096 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.1593 - acc: 0.9531 - val_loss: 0.1019 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.1705 - acc: 0.9531 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.1517 - acc: 0.9531 - val_loss: 0.0978 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.1489 - acc: 0.9688 - val_loss: 0.0944 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.1446 - acc: 0.9688 - val_loss: 0.1044 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1679 - acc: 0.9375 - val_loss: 0.0933 - val_acc: 0.9375\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 265us/step - loss: 0.1373 - acc: 0.9844 - val_loss: 0.1010 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1409 - acc: 0.9688 - val_loss: 0.0939 - val_acc: 0.9375\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.1471 - acc: 0.9531 - val_loss: 0.0887 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1362 - acc: 0.9844 - val_loss: 0.0819 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 296us/step - loss: 0.1307 - acc: 0.9531 - val_loss: 0.0780 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1261 - acc: 0.9688 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.1222 - acc: 0.9531 - val_loss: 0.0732 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1236 - acc: 0.9531 - val_loss: 0.0706 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 171us/step - loss: 0.1239 - acc: 0.9531 - val_loss: 0.0695 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.1210 - acc: 0.9688 - val_loss: 0.0746 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.1217 - acc: 0.9375 - val_loss: 0.0677 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.1132 - acc: 0.9531 - val_loss: 0.0644 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.1192 - acc: 0.9531 - val_loss: 0.0650 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1123 - acc: 0.9688 - val_loss: 0.0622 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 206us/step - loss: 0.1101 - acc: 0.9688 - val_loss: 0.0602 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 172us/step - loss: 0.1066 - acc: 0.9688 - val_loss: 0.0589 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.1054 - acc: 0.9531 - val_loss: 0.0579 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1030 - acc: 0.9688 - val_loss: 0.0567 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.1043 - acc: 0.9688 - val_loss: 0.0560 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.1055 - acc: 0.9844 - val_loss: 0.0583 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.1075 - acc: 0.9375 - val_loss: 0.0538 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 171us/step - loss: 0.1046 - acc: 0.9688 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 222us/step - loss: 0.1137 - acc: 0.9531 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.0997 - acc: 0.9531 - val_loss: 0.0511 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0997 - acc: 0.9531 - val_loss: 0.0504 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.1017 - acc: 0.9531 - val_loss: 0.0504 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.0978 - acc: 0.9844 - val_loss: 0.0485 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0985 - acc: 0.9531 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.0973 - acc: 0.9844 - val_loss: 0.0470 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0927 - acc: 0.9531 - val_loss: 0.0535 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.1234 - acc: 0.9688 - val_loss: 0.0611 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0883 - acc: 0.9688 - val_loss: 0.0710 - val_acc: 0.9375\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.1429 - acc: 0.9531 - val_loss: 0.0499 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.1140 - acc: 0.9688 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 246us/step - loss: 0.0876 - acc: 0.9688 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.0840 - acc: 0.9844 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 249us/step - loss: 0.1000 - acc: 0.9531 - val_loss: 0.0526 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 233us/step - loss: 0.1250 - acc: 0.9531 - val_loss: 0.0615 - val_acc: 0.9375\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 233us/step - loss: 0.0956 - acc: 0.9688 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 233us/step - loss: 0.0854 - acc: 0.9688 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0854 - acc: 0.9688 - val_loss: 0.0485 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 217us/step - loss: 0.0801 - acc: 0.9844 - val_loss: 0.0404 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0808 - acc: 0.9688 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0819 - acc: 0.9688 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0877 - acc: 0.9688 - val_loss: 0.0385 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 202us/step - loss: 0.0935 - acc: 0.9531 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 203us/step - loss: 0.1037 - acc: 0.9688 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 172us/step - loss: 0.1561 - acc: 0.9531 - val_loss: 0.0587 - val_acc: 0.9375\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 221us/step - loss: 0.1092 - acc: 0.9531 - val_loss: 0.0368 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.0798 - acc: 0.9688 - val_loss: 0.0413 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.0790 - acc: 0.9688 - val_loss: 0.0365 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0809 - acc: 0.9688 - val_loss: 0.0369 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 187us/step - loss: 0.0813 - acc: 0.9531 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 234us/step - loss: 0.0824 - acc: 0.9688 - val_loss: 0.0438 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 218us/step - loss: 0.0982 - acc: 0.9531 - val_loss: 0.0475 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238c0322988>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "lr = 0.01\n",
    "batch_size = 5\n",
    "num_epochs = 100\n",
    "\n",
    "n_input = X_train.shape[1]     #入力層のノード数\n",
    "n_samples = X_train.shape[0]   #サンプル数\n",
    "n_output = y_train.shape[1]    #出力層のノード数\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=n_input))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(n_output))\n",
    "model.add(Activation('sigmoid')) \n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(lr),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          batch_size=batch_size,epochs=num_epochs,verbose=1,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14394626021385193\n",
      "accuracy: 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "#print(\"y_pred_proba:\\n\", y_pred_proba,'\\n')\n",
    "#print(\"y_pred:\\n\", y_pred,'\\n')\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('loss:', score[0])\n",
    "print('accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（多値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (96, 4)\n",
      "y_train (120, 1)\n",
      "y_train_OH (96, 3)\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miyas\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv('Iris.csv')\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis] #整数型に変えて，１列に変換\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 正解データをOne-Hot表現に\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='error', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, :])\n",
    "y_test_one_hot = enc.transform(y_test[:, :])\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train_OH, y_val_OH = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)\n",
    "\n",
    "#shapeの確認\n",
    "print('X_train',X_train.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_train_OH',y_train_OH.shape)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "lr = 0.001\n",
    "batch_size = 5\n",
    "num_epochs = 100\n",
    "\n",
    "n_input = X_train.shape[1]     #入力層のノード数\n",
    "n_samples = X_train.shape[0]   #サンプル数\n",
    "n_output = y_train_OH.shape[1]    #出力層のノード数\n",
    "print(n_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 8.7310 - acc: 0.3229 - val_loss: 8.5667 - val_acc: 0.3333\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 7.9083 - acc: 0.3229 - val_loss: 7.5179 - val_acc: 0.3333\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 0s 176us/step - loss: 6.4836 - acc: 0.2500 - val_loss: 5.5633 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 0s 194us/step - loss: 4.5591 - acc: 0.0000e+00 - val_loss: 3.8518 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 3.2294 - acc: 0.0417 - val_loss: 2.7486 - val_acc: 0.0833\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 0s 177us/step - loss: 2.3904 - acc: 0.2708 - val_loss: 2.0525 - val_acc: 0.2500\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 1.8495 - acc: 0.2917 - val_loss: 1.6209 - val_acc: 0.2500\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 1.5489 - acc: 0.1354 - val_loss: 1.3923 - val_acc: 0.3333\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 0s 249us/step - loss: 1.4021 - acc: 0.2604 - val_loss: 1.2711 - val_acc: 0.4167\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 1.2847 - acc: 0.3333 - val_loss: 1.1604 - val_acc: 0.4167\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 1.1842 - acc: 0.3333 - val_loss: 1.0616 - val_acc: 0.4167\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 0s 249us/step - loss: 1.0932 - acc: 0.3542 - val_loss: 0.9751 - val_acc: 0.4167\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 1.0191 - acc: 0.3646 - val_loss: 0.9065 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.9603 - acc: 0.4896 - val_loss: 0.8523 - val_acc: 0.7917\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.8942 - acc: 0.6563 - val_loss: 0.7964 - val_acc: 0.8333\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.8464 - acc: 0.6771 - val_loss: 0.7480 - val_acc: 0.7917\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.8065 - acc: 0.6667 - val_loss: 0.7067 - val_acc: 0.7917\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 0s 249us/step - loss: 0.7637 - acc: 0.6667 - val_loss: 0.6736 - val_acc: 0.7917\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.7327 - acc: 0.6771 - val_loss: 0.6450 - val_acc: 0.7917\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.7057 - acc: 0.6979 - val_loss: 0.6226 - val_acc: 0.8333\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.6837 - acc: 0.6979 - val_loss: 0.5973 - val_acc: 0.7917\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.6618 - acc: 0.7083 - val_loss: 0.5819 - val_acc: 0.8333\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.6412 - acc: 0.7188 - val_loss: 0.5623 - val_acc: 0.8333\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.6248 - acc: 0.7083 - val_loss: 0.5487 - val_acc: 0.8333\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.6182 - acc: 0.6875 - val_loss: 0.5419 - val_acc: 0.8333\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 0s 249us/step - loss: 0.6000 - acc: 0.7083 - val_loss: 0.5279 - val_acc: 0.8333\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.5869 - acc: 0.7292 - val_loss: 0.5112 - val_acc: 0.8333\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 0s 260us/step - loss: 0.5731 - acc: 0.7292 - val_loss: 0.5048 - val_acc: 0.8750\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.5668 - acc: 0.7292 - val_loss: 0.4934 - val_acc: 0.8750\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.5546 - acc: 0.7396 - val_loss: 0.4861 - val_acc: 0.8750\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.5452 - acc: 0.7292 - val_loss: 0.4767 - val_acc: 0.8750\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.5389 - acc: 0.7500 - val_loss: 0.4705 - val_acc: 0.8750\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 0s 229us/step - loss: 0.5308 - acc: 0.7188 - val_loss: 0.4619 - val_acc: 0.8333\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.5221 - acc: 0.7500 - val_loss: 0.4558 - val_acc: 0.8750\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.5149 - acc: 0.7500 - val_loss: 0.4540 - val_acc: 0.9167\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 0s 229us/step - loss: 0.5049 - acc: 0.7604 - val_loss: 0.4463 - val_acc: 0.9167\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.4998 - acc: 0.7500 - val_loss: 0.4390 - val_acc: 0.9167\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.4908 - acc: 0.7604 - val_loss: 0.4320 - val_acc: 0.8750\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.4850 - acc: 0.7813 - val_loss: 0.4300 - val_acc: 0.9167\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.4776 - acc: 0.8021 - val_loss: 0.4243 - val_acc: 0.9167\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 0s 291us/step - loss: 0.4698 - acc: 0.7813 - val_loss: 0.4197 - val_acc: 0.9583\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.4635 - acc: 0.8125 - val_loss: 0.4130 - val_acc: 0.9167\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.4581 - acc: 0.7917 - val_loss: 0.4110 - val_acc: 0.9583\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.4514 - acc: 0.8229 - val_loss: 0.4076 - val_acc: 0.9167\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.4447 - acc: 0.8333 - val_loss: 0.4005 - val_acc: 0.9583\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.4398 - acc: 0.8229 - val_loss: 0.3952 - val_acc: 0.9583\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 0s 228us/step - loss: 0.4357 - acc: 0.8438 - val_loss: 0.3922 - val_acc: 0.9583\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 0s 177us/step - loss: 0.4277 - acc: 0.8750 - val_loss: 0.3896 - val_acc: 0.9583\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.4239 - acc: 0.8542 - val_loss: 0.3826 - val_acc: 0.9583\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.4231 - acc: 0.8854 - val_loss: 0.3844 - val_acc: 0.9167\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.4126 - acc: 0.9271 - val_loss: 0.3824 - val_acc: 0.8750\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.4070 - acc: 0.9167 - val_loss: 0.3726 - val_acc: 0.9583\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 0s 177us/step - loss: 0.4014 - acc: 0.8958 - val_loss: 0.3688 - val_acc: 0.9583\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.3952 - acc: 0.8958 - val_loss: 0.3646 - val_acc: 0.9583\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.3908 - acc: 0.8854 - val_loss: 0.3601 - val_acc: 0.9583\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.3942 - acc: 0.9063 - val_loss: 0.3615 - val_acc: 0.9167\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.3787 - acc: 0.9063 - val_loss: 0.3534 - val_acc: 0.9583\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.3779 - acc: 0.9063 - val_loss: 0.3502 - val_acc: 0.9583\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.3708 - acc: 0.9063 - val_loss: 0.3476 - val_acc: 0.9583\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.3655 - acc: 0.9375 - val_loss: 0.3439 - val_acc: 0.9583\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.3630 - acc: 0.9063 - val_loss: 0.3421 - val_acc: 0.9583\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 0s 218us/step - loss: 0.3600 - acc: 0.8958 - val_loss: 0.3454 - val_acc: 0.8750\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 0s 239us/step - loss: 0.3513 - acc: 0.9271 - val_loss: 0.3359 - val_acc: 0.9583\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.3476 - acc: 0.9375 - val_loss: 0.3312 - val_acc: 0.9583\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.3424 - acc: 0.9375 - val_loss: 0.3280 - val_acc: 0.9583\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.3389 - acc: 0.9375 - val_loss: 0.3255 - val_acc: 0.9583\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 0s 281us/step - loss: 0.3335 - acc: 0.9375 - val_loss: 0.3223 - val_acc: 0.9583\n",
      "Epoch 68/100\n",
      "96/96 [==============================] - 0s 386us/step - loss: 0.3286 - acc: 0.9375 - val_loss: 0.3205 - val_acc: 0.9583\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 0s 415us/step - loss: 0.3249 - acc: 0.9375 - val_loss: 0.3192 - val_acc: 0.9167\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 0s 207us/step - loss: 0.3221 - acc: 0.9375 - val_loss: 0.3152 - val_acc: 0.9583\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 0s 196us/step - loss: 0.3182 - acc: 0.9375 - val_loss: 0.3135 - val_acc: 0.9583\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 0s 221us/step - loss: 0.3190 - acc: 0.9375 - val_loss: 0.3091 - val_acc: 0.9583\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 0s 177us/step - loss: 0.3086 - acc: 0.9375 - val_loss: 0.3070 - val_acc: 0.9583\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 0s 219us/step - loss: 0.3065 - acc: 0.9479 - val_loss: 0.3048 - val_acc: 0.9583\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.3173 - acc: 0.9167 - val_loss: 0.3009 - val_acc: 0.9583\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.3019 - acc: 0.9271 - val_loss: 0.3078 - val_acc: 0.8750\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2994 - acc: 0.9375 - val_loss: 0.2962 - val_acc: 0.9583\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.2885 - acc: 0.9479 - val_loss: 0.2993 - val_acc: 0.9167\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2856 - acc: 0.9479 - val_loss: 0.2928 - val_acc: 0.9583\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2822 - acc: 0.9479 - val_loss: 0.2904 - val_acc: 0.9583\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2773 - acc: 0.9479 - val_loss: 0.2865 - val_acc: 0.9583\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 0s 183us/step - loss: 0.2732 - acc: 0.9479 - val_loss: 0.2871 - val_acc: 0.9167\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2705 - acc: 0.9479 - val_loss: 0.2826 - val_acc: 0.9583\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2678 - acc: 0.9479 - val_loss: 0.2800 - val_acc: 0.9583\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 0s 193us/step - loss: 0.2660 - acc: 0.9479 - val_loss: 0.2791 - val_acc: 0.9583\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 0s 185us/step - loss: 0.2632 - acc: 0.9479 - val_loss: 0.2782 - val_acc: 0.9167\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 0s 196us/step - loss: 0.2578 - acc: 0.9479 - val_loss: 0.2778 - val_acc: 0.9167\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.2583 - acc: 0.9479 - val_loss: 0.2785 - val_acc: 0.9167\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2541 - acc: 0.9479 - val_loss: 0.2723 - val_acc: 0.9583\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.2493 - acc: 0.9479 - val_loss: 0.2694 - val_acc: 0.9583\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 0s 199us/step - loss: 0.2451 - acc: 0.9583 - val_loss: 0.2688 - val_acc: 0.9583\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.2421 - acc: 0.9479 - val_loss: 0.2664 - val_acc: 0.9583\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.2395 - acc: 0.9479 - val_loss: 0.2635 - val_acc: 0.9583\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 0s 209us/step - loss: 0.2410 - acc: 0.9479 - val_loss: 0.2622 - val_acc: 0.9583\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 0s 195us/step - loss: 0.2376 - acc: 0.9479 - val_loss: 0.2616 - val_acc: 0.9583\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 0s 187us/step - loss: 0.2308 - acc: 0.9583 - val_loss: 0.2620 - val_acc: 0.9167\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.2289 - acc: 0.9688 - val_loss: 0.2584 - val_acc: 0.9583\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.2272 - acc: 0.9479 - val_loss: 0.2557 - val_acc: 0.9583\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 0s 208us/step - loss: 0.2255 - acc: 0.9479 - val_loss: 0.2544 - val_acc: 0.9583\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 0s 197us/step - loss: 0.2202 - acc: 0.9583 - val_loss: 0.2528 - val_acc: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238c305a348>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#活性化関数にReLUを使うので，重みの初期化にはHeを使う\n",
    "from keras.initializers import he_normal\n",
    "\n",
    "K.clear_session()#レイヤの通し番号がつながるの防止\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=n_input, kernel_initializer=he_normal()))#入力層→隠れ層\n",
    "model.add(Activation('relu')) #活性化層\n",
    "model.add(Dense(n_output))    #隠れ層→出力層\n",
    "model.add(Activation('softmax')) \n",
    "\n",
    "model.summary()\n",
    "#コンパイル（多値なのでlossはcategorical_crossentropy）\n",
    "model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#学習\n",
    "model.fit(X_train,y_train_OH,\n",
    "          batch_size=batch_size,epochs=num_epochs,verbose=1,\n",
    "          validation_data=(X_val, y_val_OH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_proba:\n",
      " [[2.3337937e-04 1.2709251e-01 8.7267411e-01]\n",
      " [1.1964528e-02 5.9248936e-01 3.9554605e-01]\n",
      " [9.9433857e-01 5.6614815e-03 5.4703424e-08]\n",
      " [8.1532497e-05 1.0334647e-01 8.9657199e-01]\n",
      " [9.7300816e-01 2.6988748e-02 3.0967667e-06]] \n",
      "\n",
      "y_pred:\n",
      " [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0] \n",
      "\n",
      "loss: 0.22005607187747955\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_proba,axis=1)\n",
    "print(\"y_proba:\\n\", y_proba[:5],'\\n')\n",
    "print(\"y_pred:\\n\", y_pred,'\\n')\n",
    "\n",
    "score = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print('loss:', score[0])\n",
    "print('accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】House PricesをKerasで学習\n",
    "## データを準備\n",
    "### １．特徴量を２変数のみに絞り，対数変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (1460, 2)\n",
      "y_shape: (1460, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/miyas/kaggle/train.csv')\n",
    "X = df.loc[:,['GrLivArea','YearBuilt']].values\n",
    "X_log = np.log(X)\n",
    "y = df.SalePrice.values\n",
    "y_log = np.log(y).reshape(-1,1)\n",
    "print('X_shape:',X_log.shape)\n",
    "print('y_shape:',y_log.shape)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log, y_log, test_size=0.2, random_state=0)\n",
    "X_train_log, X_val_log, y_train_log, y_val_log = train_test_split(X_train_log, y_train_log, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### １．特徴量を２変数のみに絞り，標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_val_std = sc.transform(X_val)\n",
    "sc.fit(y_train[:,np.newaxis])\n",
    "y_train_std = sc.transform(y_train[:,np.newaxis])\n",
    "y_test_std = sc.transform(y_test[:,np.newaxis])\n",
    "y_val_std = sc.transform(y_val[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ２．特徴量は欠損値とobject型を削除し，標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  MSZoning  LotArea  Street  LotShape  LandContour  \\\n",
       "0   0           0         0        0       0         0            0   \n",
       "\n",
       "   Utilities  LotConfig  LandSlope  ...  EnclosedPorch  3SsnPorch  \\\n",
       "0          0          0          0  ...              0          0   \n",
       "\n",
       "   ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SaleType  SaleCondition  \\\n",
       "0            0         0        0       0       0         0              0   \n",
       "\n",
       "   SalePrice  \n",
       "0          0  \n",
       "\n",
       "[1 rows x 62 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データ２　欠損値とオブジェクト型は削除して対数変換\n",
    "#今回は欠損値のある特徴量は削除\n",
    "df2 = df.dropna(how='any',axis=1)\n",
    "pd.DataFrame(df2.isna().sum()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSZoning          5\n",
       "Street            2\n",
       "LotShape          4\n",
       "LandContour       4\n",
       "Utilities         2\n",
       "LotConfig         5\n",
       "LandSlope         3\n",
       "Neighborhood     25\n",
       "Condition1        9\n",
       "Condition2        8\n",
       "BldgType          5\n",
       "HouseStyle        8\n",
       "RoofStyle         6\n",
       "RoofMatl          8\n",
       "Exterior1st      15\n",
       "Exterior2nd      16\n",
       "ExterQual         4\n",
       "ExterCond         5\n",
       "Foundation        6\n",
       "Heating           6\n",
       "HeatingQC         5\n",
       "CentralAir        2\n",
       "KitchenQual       4\n",
       "Functional        7\n",
       "PavedDrive        3\n",
       "SaleType          9\n",
       "SaleCondition     6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# object型の個数\n",
    "object_count = df2.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n",
    "object_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 35)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_col = df2.select_dtypes('object')\n",
    "data = df2.drop(object_col,axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (934, 34)\n",
      "y_shape (234, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('SalePrice',axis=1).values\n",
    "y = data.SalePrice.values\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std_1 = sc.transform(X_train)\n",
    "X_test_std_1 = sc.transform(X_test)\n",
    "X_val_std_1 = sc.transform(X_val)\n",
    "sc.fit(y_train[:,np.newaxis])\n",
    "y_train_std_1 = sc.transform(y_train[:,np.newaxis])\n",
    "y_test_std_1 = sc.transform(y_test[:,np.newaxis])\n",
    "y_val_std_1 = sc.transform(y_val[:,np.newaxis])\n",
    "\n",
    "print('X_shape',X_train_std_1.shape)\n",
    "print('y_shape',y_val_std_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ３．欠損列は削除，object型はOne-Hotしたりして標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 columns were label encoded.\n",
      "Features shape:  (1460, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miyas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ラベルエンコーダーをインスタンス化\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in df2:\n",
    "    if df2[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(df2[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(df2[col])\n",
    "            # Transform both training and test_data2ing data\n",
    "            df2[col] = le.transform(df2[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)\n",
    "print('Features shape: ', df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (1460, 190)\n"
     ]
    }
   ],
   "source": [
    "# one-hotエンコーディング（１列目は削除）\n",
    "df3 = pd.get_dummies(df2,drop_first=True)\n",
    "print('Features shape: ', df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (934, 189)\n",
      "y_shape (234, 1)\n"
     ]
    }
   ],
   "source": [
    "df3 = df3.astype(float)\n",
    "X = df3.drop('SalePrice',axis=1).values\n",
    "y = df3.SalePrice.values\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std_2 = sc.transform(X_train)\n",
    "X_test_std_2 = sc.transform(X_test)\n",
    "X_val_std_2 = sc.transform(X_val)\n",
    "sc.fit(y_train[:,np.newaxis])\n",
    "y_train_std_2 = sc.transform(y_train[:,np.newaxis])\n",
    "y_test_std_2 = sc.transform(y_test[:,np.newaxis])\n",
    "y_val_std_2 = sc.transform(y_val[:,np.newaxis])\n",
    "\n",
    "print('X_shape',X_train_std_2.shape)\n",
    "print('y_shape',y_val_std_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "lr = 0.001\n",
    "batch_size = 20\n",
    "num_epochs = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liner_model(X,y, X_val,y_val,X_test,y_test):\n",
    "    n_input = X.shape[1]     #入力層のノード数\n",
    "    n_output = y.shape[1]    #出力層のノード数\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=n_input,  kernel_initializer=he_normal()))#入力層→隠れ層\n",
    "    model.add(Activation('relu')) #活性化層\n",
    "    model.add(Dense(n_output, kernel_initializer=he_normal()))    #隠れ層→出力層\n",
    "    model.add(Activation('relu')) \n",
    "\n",
    "    model.summary()\n",
    "    #コンパイル(metricsは評価関数)\n",
    "    model.compile(optimizer=Adam(lr), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    #学習\n",
    "    model.fit(X, y, batch_size=batch_size, epochs=num_epochs, verbose=1,\n",
    "              validation_data=(X_val, y_val))\n",
    "    \n",
    "    # 推定\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('loss(MSE):', score[0])\n",
    "    print('MAE:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/100\n",
      "934/934 [==============================] - 0s 191us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 2/100\n",
      "934/934 [==============================] - 0s 32us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 3/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 4/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 5/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 6/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 7/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 8/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 9/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 10/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 11/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 12/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 13/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 14/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 15/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 16/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 17/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 18/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 19/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 20/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 21/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 22/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 23/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 24/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 25/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 26/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 27/100\n",
      "934/934 [==============================] - 0s 51us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 28/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 29/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 30/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 31/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 32/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 33/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 34/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 35/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 36/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 37/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 38/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 39/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 40/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 41/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 42/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 43/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 44/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 45/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 46/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 47/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 48/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 49/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 50/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 51/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 52/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 53/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 54/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 55/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 56/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 57/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 58/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 59/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 60/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 61/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 62/100\n",
      "934/934 [==============================] - 0s 49us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 63/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 64/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 65/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 66/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 67/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 68/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 69/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 70/100\n",
      "934/934 [==============================] - 0s 49us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 71/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 72/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 73/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 74/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 75/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 76/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 77/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 78/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 79/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 80/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 81/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 82/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 83/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 84/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 85/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 86/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 87/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 88/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 89/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 91/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 92/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 93/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 94/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 95/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 96/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 97/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 98/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 99/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "Epoch 100/100\n",
      "934/934 [==============================] - 0s 32us/step - loss: 144.7989 - mean_absolute_error: 12.0265 - val_loss: 144.4178 - val_mean_absolute_error: 12.0109\n",
      "loss(MSE): 144.79612857348297\n",
      "MAE: 12.026833730201199\n"
     ]
    }
   ],
   "source": [
    "#特徴量２変数で対数変換\n",
    "liner_model(X_train_log,y_train_log,X_val_log,y_val_log,X_test_log,y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/100\n",
      "934/934 [==============================] - 0s 220us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 2/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 3/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 4/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 5/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 6/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 7/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 8/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 9/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 10/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 11/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 12/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 13/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 14/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 15/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 16/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 17/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 18/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 19/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 20/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 21/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 22/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 23/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 24/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 25/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 26/100\n",
      "934/934 [==============================] - 0s 54us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 27/100\n",
      "934/934 [==============================] - 0s 53us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 28/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 29/100\n",
      "934/934 [==============================] - 0s 52us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 30/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 31/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 32/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 33/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 34/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 35/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 36/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 37/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 38/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 39/100\n",
      "934/934 [==============================] - 0s 33us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 40/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 41/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 42/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 43/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 44/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 46/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 47/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 48/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 49/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 50/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 51/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 52/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 53/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 54/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 55/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 56/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 57/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 58/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 59/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 60/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 61/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 62/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 63/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 64/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 65/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 66/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 67/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 68/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 69/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 70/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 71/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 72/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 73/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 74/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 75/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 76/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 77/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 78/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 79/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 80/100\n",
      "934/934 [==============================] - 0s 33us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 81/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 82/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 83/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 84/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 85/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 86/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 87/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 88/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 89/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 90/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 91/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 92/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 93/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 37us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 95/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 96/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 97/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 98/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 99/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "Epoch 100/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 1.0000 - mean_absolute_error: 0.7303 - val_loss: 0.8704 - val_mean_absolute_error: 0.7059\n",
      "loss(MSE): 1.0924174295712823\n",
      "MAE: 0.7195033275917785\n"
     ]
    }
   ],
   "source": [
    "#特徴量２変数で標準化\n",
    "liner_model(X_train_std,y_train_std,X_val_std,y_val_std,X_test_std,y_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                560       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 577\n",
      "Trainable params: 577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/100\n",
      "934/934 [==============================] - 0s 189us/step - loss: 1.0664 - mean_absolute_error: 0.8285 - val_loss: 0.8361 - val_mean_absolute_error: 0.7271\n",
      "Epoch 2/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.7213 - mean_absolute_error: 0.6865 - val_loss: 0.6593 - val_mean_absolute_error: 0.6426\n",
      "Epoch 3/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.5899 - mean_absolute_error: 0.6194 - val_loss: 0.5883 - val_mean_absolute_error: 0.6089\n",
      "Epoch 4/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.5310 - mean_absolute_error: 0.5827 - val_loss: 0.5395 - val_mean_absolute_error: 0.5794\n",
      "Epoch 5/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.4940 - mean_absolute_error: 0.5581 - val_loss: 0.5088 - val_mean_absolute_error: 0.5645\n",
      "Epoch 6/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.4694 - mean_absolute_error: 0.5425 - val_loss: 0.4874 - val_mean_absolute_error: 0.5500\n",
      "Epoch 7/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.4528 - mean_absolute_error: 0.5308 - val_loss: 0.4728 - val_mean_absolute_error: 0.5467\n",
      "Epoch 8/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.4424 - mean_absolute_error: 0.5230 - val_loss: 0.4625 - val_mean_absolute_error: 0.5422\n",
      "Epoch 9/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.4322 - mean_absolute_error: 0.5153 - val_loss: 0.4546 - val_mean_absolute_error: 0.5355\n",
      "Epoch 10/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.4295 - mean_absolute_error: 0.5124 - val_loss: 0.4484 - val_mean_absolute_error: 0.5269\n",
      "Epoch 11/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.4186 - mean_absolute_error: 0.5064 - val_loss: 0.4400 - val_mean_absolute_error: 0.5271\n",
      "Epoch 12/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.4165 - mean_absolute_error: 0.5025 - val_loss: 0.4371 - val_mean_absolute_error: 0.5260\n",
      "Epoch 13/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.4128 - mean_absolute_error: 0.4993 - val_loss: 0.4343 - val_mean_absolute_error: 0.5259\n",
      "Epoch 14/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.4099 - mean_absolute_error: 0.4981 - val_loss: 0.4376 - val_mean_absolute_error: 0.5212\n",
      "Epoch 15/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 0.4060 - mean_absolute_error: 0.4931 - val_loss: 0.4345 - val_mean_absolute_error: 0.5195\n",
      "Epoch 16/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.4026 - mean_absolute_error: 0.4911 - val_loss: 0.4291 - val_mean_absolute_error: 0.5183\n",
      "Epoch 17/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.4038 - mean_absolute_error: 0.4928 - val_loss: 0.4274 - val_mean_absolute_error: 0.5181\n",
      "Epoch 18/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.3989 - mean_absolute_error: 0.4902 - val_loss: 0.4308 - val_mean_absolute_error: 0.5160\n",
      "Epoch 19/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3978 - mean_absolute_error: 0.4893 - val_loss: 0.4260 - val_mean_absolute_error: 0.5140\n",
      "Epoch 20/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3968 - mean_absolute_error: 0.4887 - val_loss: 0.4303 - val_mean_absolute_error: 0.5150\n",
      "Epoch 21/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3947 - mean_absolute_error: 0.4893 - val_loss: 0.4262 - val_mean_absolute_error: 0.5165\n",
      "Epoch 22/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3906 - mean_absolute_error: 0.4853 - val_loss: 0.4239 - val_mean_absolute_error: 0.5191\n",
      "Epoch 23/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 0.3919 - mean_absolute_error: 0.4849 - val_loss: 0.4248 - val_mean_absolute_error: 0.5131\n",
      "Epoch 24/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.3885 - mean_absolute_error: 0.4832 - val_loss: 0.4250 - val_mean_absolute_error: 0.5141\n",
      "Epoch 25/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3883 - mean_absolute_error: 0.4846 - val_loss: 0.4276 - val_mean_absolute_error: 0.5153\n",
      "Epoch 26/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3870 - mean_absolute_error: 0.4846 - val_loss: 0.4215 - val_mean_absolute_error: 0.5183\n",
      "Epoch 27/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3847 - mean_absolute_error: 0.4825 - val_loss: 0.4222 - val_mean_absolute_error: 0.5192\n",
      "Epoch 28/100\n",
      "934/934 [==============================] - 0s 57us/step - loss: 0.3836 - mean_absolute_error: 0.4805 - val_loss: 0.4216 - val_mean_absolute_error: 0.5182\n",
      "Epoch 29/100\n",
      "934/934 [==============================] - 0s 56us/step - loss: 0.3812 - mean_absolute_error: 0.4787 - val_loss: 0.4228 - val_mean_absolute_error: 0.5189\n",
      "Epoch 30/100\n",
      "934/934 [==============================] - 0s 51us/step - loss: 0.3856 - mean_absolute_error: 0.4851 - val_loss: 0.4313 - val_mean_absolute_error: 0.5188\n",
      "Epoch 31/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3822 - mean_absolute_error: 0.4774 - val_loss: 0.4231 - val_mean_absolute_error: 0.5159\n",
      "Epoch 32/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3795 - mean_absolute_error: 0.4761 - val_loss: 0.4202 - val_mean_absolute_error: 0.5170\n",
      "Epoch 33/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3782 - mean_absolute_error: 0.4769 - val_loss: 0.4248 - val_mean_absolute_error: 0.5179\n",
      "Epoch 34/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3753 - mean_absolute_error: 0.4761 - val_loss: 0.4240 - val_mean_absolute_error: 0.5232\n",
      "Epoch 35/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 0.3775 - mean_absolute_error: 0.4771 - val_loss: 0.4213 - val_mean_absolute_error: 0.5194\n",
      "Epoch 36/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3764 - mean_absolute_error: 0.4752 - val_loss: 0.4204 - val_mean_absolute_error: 0.5190\n",
      "Epoch 37/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3781 - mean_absolute_error: 0.4737 - val_loss: 0.4214 - val_mean_absolute_error: 0.5205\n",
      "Epoch 38/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3724 - mean_absolute_error: 0.4739 - val_loss: 0.4260 - val_mean_absolute_error: 0.5202\n",
      "Epoch 39/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3733 - mean_absolute_error: 0.4749 - val_loss: 0.4212 - val_mean_absolute_error: 0.5219\n",
      "Epoch 40/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3704 - mean_absolute_error: 0.4719 - val_loss: 0.4196 - val_mean_absolute_error: 0.5191\n",
      "Epoch 41/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 0.3697 - mean_absolute_error: 0.4712 - val_loss: 0.4218 - val_mean_absolute_error: 0.5199\n",
      "Epoch 42/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3692 - mean_absolute_error: 0.4703 - val_loss: 0.4247 - val_mean_absolute_error: 0.5201\n",
      "Epoch 43/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3680 - mean_absolute_error: 0.4698 - val_loss: 0.4216 - val_mean_absolute_error: 0.5224\n",
      "Epoch 44/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 0.3686 - mean_absolute_error: 0.4696 - val_loss: 0.4224 - val_mean_absolute_error: 0.5219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3669 - mean_absolute_error: 0.4682 - val_loss: 0.4208 - val_mean_absolute_error: 0.5217\n",
      "Epoch 46/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3662 - mean_absolute_error: 0.4693 - val_loss: 0.4272 - val_mean_absolute_error: 0.5225\n",
      "Epoch 47/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 0.3656 - mean_absolute_error: 0.4682 - val_loss: 0.4302 - val_mean_absolute_error: 0.5247\n",
      "Epoch 48/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3630 - mean_absolute_error: 0.4669 - val_loss: 0.4230 - val_mean_absolute_error: 0.5251\n",
      "Epoch 49/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3632 - mean_absolute_error: 0.4657 - val_loss: 0.4224 - val_mean_absolute_error: 0.5237\n",
      "Epoch 50/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3633 - mean_absolute_error: 0.4662 - val_loss: 0.4302 - val_mean_absolute_error: 0.5254\n",
      "Epoch 51/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3621 - mean_absolute_error: 0.4638 - val_loss: 0.4251 - val_mean_absolute_error: 0.5227\n",
      "Epoch 52/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3593 - mean_absolute_error: 0.4624 - val_loss: 0.4197 - val_mean_absolute_error: 0.5229\n",
      "Epoch 53/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3592 - mean_absolute_error: 0.4622 - val_loss: 0.4278 - val_mean_absolute_error: 0.5248\n",
      "Epoch 54/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3593 - mean_absolute_error: 0.4624 - val_loss: 0.4256 - val_mean_absolute_error: 0.5235\n",
      "Epoch 55/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3575 - mean_absolute_error: 0.4612 - val_loss: 0.4245 - val_mean_absolute_error: 0.5249\n",
      "Epoch 56/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3580 - mean_absolute_error: 0.4601 - val_loss: 0.4230 - val_mean_absolute_error: 0.5240\n",
      "Epoch 57/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3571 - mean_absolute_error: 0.4607 - val_loss: 0.4266 - val_mean_absolute_error: 0.5250\n",
      "Epoch 58/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 0.3582 - mean_absolute_error: 0.4598 - val_loss: 0.4255 - val_mean_absolute_error: 0.5247\n",
      "Epoch 59/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3561 - mean_absolute_error: 0.4608 - val_loss: 0.4299 - val_mean_absolute_error: 0.5269\n",
      "Epoch 60/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3576 - mean_absolute_error: 0.4604 - val_loss: 0.4271 - val_mean_absolute_error: 0.5264\n",
      "Epoch 61/100\n",
      "934/934 [==============================] - 0s 36us/step - loss: 0.3557 - mean_absolute_error: 0.4588 - val_loss: 0.4255 - val_mean_absolute_error: 0.5261\n",
      "Epoch 62/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3544 - mean_absolute_error: 0.4585 - val_loss: 0.4235 - val_mean_absolute_error: 0.5253\n",
      "Epoch 63/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3527 - mean_absolute_error: 0.4568 - val_loss: 0.4249 - val_mean_absolute_error: 0.5281\n",
      "Epoch 64/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 0.3533 - mean_absolute_error: 0.4598 - val_loss: 0.4225 - val_mean_absolute_error: 0.5257\n",
      "Epoch 65/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 0.3491 - mean_absolute_error: 0.4551 - val_loss: 0.4251 - val_mean_absolute_error: 0.5271\n",
      "Epoch 66/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3487 - mean_absolute_error: 0.4543 - val_loss: 0.4231 - val_mean_absolute_error: 0.5268\n",
      "Epoch 67/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3479 - mean_absolute_error: 0.4539 - val_loss: 0.4216 - val_mean_absolute_error: 0.5284\n",
      "Epoch 68/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3472 - mean_absolute_error: 0.4543 - val_loss: 0.4239 - val_mean_absolute_error: 0.5282\n",
      "Epoch 69/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3461 - mean_absolute_error: 0.4518 - val_loss: 0.4248 - val_mean_absolute_error: 0.5292\n",
      "Epoch 70/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3462 - mean_absolute_error: 0.4516 - val_loss: 0.4244 - val_mean_absolute_error: 0.5287\n",
      "Epoch 71/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3447 - mean_absolute_error: 0.4507 - val_loss: 0.4266 - val_mean_absolute_error: 0.5290\n",
      "Epoch 72/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3442 - mean_absolute_error: 0.4520 - val_loss: 0.4268 - val_mean_absolute_error: 0.5307\n",
      "Epoch 73/100\n",
      "934/934 [==============================] - 0s 35us/step - loss: 0.3448 - mean_absolute_error: 0.4532 - val_loss: 0.4271 - val_mean_absolute_error: 0.5304\n",
      "Epoch 74/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3440 - mean_absolute_error: 0.4505 - val_loss: 0.4250 - val_mean_absolute_error: 0.5295\n",
      "Epoch 75/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3429 - mean_absolute_error: 0.4498 - val_loss: 0.4291 - val_mean_absolute_error: 0.5302\n",
      "Epoch 76/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3419 - mean_absolute_error: 0.4487 - val_loss: 0.4269 - val_mean_absolute_error: 0.5297\n",
      "Epoch 77/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3427 - mean_absolute_error: 0.4501 - val_loss: 0.4260 - val_mean_absolute_error: 0.5306\n",
      "Epoch 78/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3410 - mean_absolute_error: 0.4494 - val_loss: 0.4305 - val_mean_absolute_error: 0.5309\n",
      "Epoch 79/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 0.3398 - mean_absolute_error: 0.4485 - val_loss: 0.4255 - val_mean_absolute_error: 0.5302\n",
      "Epoch 80/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3404 - mean_absolute_error: 0.4480 - val_loss: 0.4312 - val_mean_absolute_error: 0.5312\n",
      "Epoch 81/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3397 - mean_absolute_error: 0.4493 - val_loss: 0.4266 - val_mean_absolute_error: 0.5305\n",
      "Epoch 82/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3393 - mean_absolute_error: 0.4475 - val_loss: 0.4275 - val_mean_absolute_error: 0.5312\n",
      "Epoch 83/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 0.3388 - mean_absolute_error: 0.4471 - val_loss: 0.4250 - val_mean_absolute_error: 0.5309\n",
      "Epoch 84/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3381 - mean_absolute_error: 0.4456 - val_loss: 0.4292 - val_mean_absolute_error: 0.5308\n",
      "Epoch 85/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 0.3369 - mean_absolute_error: 0.4455 - val_loss: 0.4254 - val_mean_absolute_error: 0.5307\n",
      "Epoch 86/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3368 - mean_absolute_error: 0.4450 - val_loss: 0.4278 - val_mean_absolute_error: 0.5303\n",
      "Epoch 87/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3367 - mean_absolute_error: 0.4446 - val_loss: 0.4276 - val_mean_absolute_error: 0.5304\n",
      "Epoch 88/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3376 - mean_absolute_error: 0.4456 - val_loss: 0.4309 - val_mean_absolute_error: 0.5309\n",
      "Epoch 89/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3357 - mean_absolute_error: 0.4429 - val_loss: 0.4260 - val_mean_absolute_error: 0.5299\n",
      "Epoch 90/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3352 - mean_absolute_error: 0.4431 - val_loss: 0.4264 - val_mean_absolute_error: 0.5311\n",
      "Epoch 91/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3358 - mean_absolute_error: 0.4446 - val_loss: 0.4260 - val_mean_absolute_error: 0.5302\n",
      "Epoch 92/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3354 - mean_absolute_error: 0.4438 - val_loss: 0.4243 - val_mean_absolute_error: 0.5294\n",
      "Epoch 93/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3351 - mean_absolute_error: 0.4430 - val_loss: 0.4232 - val_mean_absolute_error: 0.5293\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 47us/step - loss: 0.3349 - mean_absolute_error: 0.4431 - val_loss: 0.4296 - val_mean_absolute_error: 0.5310\n",
      "Epoch 95/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3342 - mean_absolute_error: 0.4424 - val_loss: 0.4270 - val_mean_absolute_error: 0.5307\n",
      "Epoch 96/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3342 - mean_absolute_error: 0.4423 - val_loss: 0.4271 - val_mean_absolute_error: 0.5297\n",
      "Epoch 97/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3345 - mean_absolute_error: 0.4413 - val_loss: 0.4278 - val_mean_absolute_error: 0.5297\n",
      "Epoch 98/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3336 - mean_absolute_error: 0.4412 - val_loss: 0.4290 - val_mean_absolute_error: 0.5308\n",
      "Epoch 99/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3326 - mean_absolute_error: 0.4407 - val_loss: 0.4253 - val_mean_absolute_error: 0.5286\n",
      "Epoch 100/100\n",
      "934/934 [==============================] - 0s 34us/step - loss: 0.3322 - mean_absolute_error: 0.4393 - val_loss: 0.4336 - val_mean_absolute_error: 0.5308\n",
      "loss(MSE): 0.5859949817396191\n",
      "MAE: 0.5412735449124689\n"
     ]
    }
   ],
   "source": [
    "#欠損値とobject型は削除して，標準化\n",
    "liner_model(X_train_std_1,y_train_std_1,X_val_std_1,y_val_std_1,X_test_std_1,y_test_std_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                3040      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,057\n",
      "Trainable params: 3,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/100\n",
      "934/934 [==============================] - 0s 210us/step - loss: 0.7383 - mean_absolute_error: 0.6416 - val_loss: 0.5318 - val_mean_absolute_error: 0.6004\n",
      "Epoch 2/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.5412 - mean_absolute_error: 0.5665 - val_loss: 0.4765 - val_mean_absolute_error: 0.5610\n",
      "Epoch 3/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.4669 - mean_absolute_error: 0.5344 - val_loss: 0.4483 - val_mean_absolute_error: 0.5421\n",
      "Epoch 4/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.4312 - mean_absolute_error: 0.5127 - val_loss: 0.4388 - val_mean_absolute_error: 0.5382\n",
      "Epoch 5/100\n",
      "934/934 [==============================] - 0s 54us/step - loss: 0.4092 - mean_absolute_error: 0.5001 - val_loss: 0.4368 - val_mean_absolute_error: 0.5345\n",
      "Epoch 6/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 0.3925 - mean_absolute_error: 0.4883 - val_loss: 0.4357 - val_mean_absolute_error: 0.5336\n",
      "Epoch 7/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.3814 - mean_absolute_error: 0.4811 - val_loss: 0.4345 - val_mean_absolute_error: 0.5332\n",
      "Epoch 8/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3733 - mean_absolute_error: 0.4734 - val_loss: 0.4351 - val_mean_absolute_error: 0.5348\n",
      "Epoch 9/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3691 - mean_absolute_error: 0.4704 - val_loss: 0.4337 - val_mean_absolute_error: 0.5322\n",
      "Epoch 10/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3638 - mean_absolute_error: 0.4648 - val_loss: 0.4339 - val_mean_absolute_error: 0.5339\n",
      "Epoch 11/100\n",
      "934/934 [==============================] - 0s 52us/step - loss: 0.3601 - mean_absolute_error: 0.4619 - val_loss: 0.4317 - val_mean_absolute_error: 0.5339\n",
      "Epoch 12/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3526 - mean_absolute_error: 0.4580 - val_loss: 0.4313 - val_mean_absolute_error: 0.5335\n",
      "Epoch 13/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3503 - mean_absolute_error: 0.4542 - val_loss: 0.4351 - val_mean_absolute_error: 0.5348\n",
      "Epoch 14/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3486 - mean_absolute_error: 0.4514 - val_loss: 0.4357 - val_mean_absolute_error: 0.5350\n",
      "Epoch 15/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3467 - mean_absolute_error: 0.4494 - val_loss: 0.4312 - val_mean_absolute_error: 0.5319\n",
      "Epoch 16/100\n",
      "934/934 [==============================] - 0s 53us/step - loss: 0.3441 - mean_absolute_error: 0.4471 - val_loss: 0.4302 - val_mean_absolute_error: 0.5313\n",
      "Epoch 17/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3424 - mean_absolute_error: 0.4449 - val_loss: 0.4352 - val_mean_absolute_error: 0.5337\n",
      "Epoch 18/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 0.3430 - mean_absolute_error: 0.4462 - val_loss: 0.4394 - val_mean_absolute_error: 0.5370\n",
      "Epoch 19/100\n",
      "934/934 [==============================] - 0s 66us/step - loss: 0.3413 - mean_absolute_error: 0.4442 - val_loss: 0.4329 - val_mean_absolute_error: 0.5320\n",
      "Epoch 20/100\n",
      "934/934 [==============================] - 0s 61us/step - loss: 0.3411 - mean_absolute_error: 0.4429 - val_loss: 0.4336 - val_mean_absolute_error: 0.5341\n",
      "Epoch 21/100\n",
      "934/934 [==============================] - 0s 59us/step - loss: 0.3419 - mean_absolute_error: 0.4478 - val_loss: 0.4369 - val_mean_absolute_error: 0.5332\n",
      "Epoch 22/100\n",
      "934/934 [==============================] - 0s 61us/step - loss: 0.3381 - mean_absolute_error: 0.4396 - val_loss: 0.4331 - val_mean_absolute_error: 0.5327\n",
      "Epoch 23/100\n",
      "934/934 [==============================] - 0s 53us/step - loss: 0.3367 - mean_absolute_error: 0.4370 - val_loss: 0.4374 - val_mean_absolute_error: 0.5361\n",
      "Epoch 24/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3361 - mean_absolute_error: 0.4365 - val_loss: 0.4333 - val_mean_absolute_error: 0.5343\n",
      "Epoch 25/100\n",
      "934/934 [==============================] - 0s 49us/step - loss: 0.3368 - mean_absolute_error: 0.4381 - val_loss: 0.4385 - val_mean_absolute_error: 0.5379\n",
      "Epoch 26/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3369 - mean_absolute_error: 0.4386 - val_loss: 0.4362 - val_mean_absolute_error: 0.5363\n",
      "Epoch 27/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3355 - mean_absolute_error: 0.4353 - val_loss: 0.4416 - val_mean_absolute_error: 0.5410\n",
      "Epoch 28/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.3352 - mean_absolute_error: 0.4363 - val_loss: 0.4395 - val_mean_absolute_error: 0.5381\n",
      "Epoch 29/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3358 - mean_absolute_error: 0.4362 - val_loss: 0.4406 - val_mean_absolute_error: 0.5400\n",
      "Epoch 30/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3351 - mean_absolute_error: 0.4341 - val_loss: 0.4351 - val_mean_absolute_error: 0.5366\n",
      "Epoch 31/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3337 - mean_absolute_error: 0.4327 - val_loss: 0.4385 - val_mean_absolute_error: 0.5390\n",
      "Epoch 32/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3337 - mean_absolute_error: 0.4319 - val_loss: 0.4398 - val_mean_absolute_error: 0.5379\n",
      "Epoch 33/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3333 - mean_absolute_error: 0.4310 - val_loss: 0.4387 - val_mean_absolute_error: 0.5403\n",
      "Epoch 34/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3322 - mean_absolute_error: 0.4299 - val_loss: 0.4393 - val_mean_absolute_error: 0.5407\n",
      "Epoch 35/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3320 - mean_absolute_error: 0.4306 - val_loss: 0.4404 - val_mean_absolute_error: 0.5404\n",
      "Epoch 36/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3310 - mean_absolute_error: 0.4276 - val_loss: 0.4430 - val_mean_absolute_error: 0.5421\n",
      "Epoch 37/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3312 - mean_absolute_error: 0.4283 - val_loss: 0.4410 - val_mean_absolute_error: 0.5420\n",
      "Epoch 38/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3311 - mean_absolute_error: 0.4295 - val_loss: 0.4400 - val_mean_absolute_error: 0.5408\n",
      "Epoch 39/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3309 - mean_absolute_error: 0.4286 - val_loss: 0.4462 - val_mean_absolute_error: 0.5438\n",
      "Epoch 40/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3312 - mean_absolute_error: 0.4304 - val_loss: 0.4422 - val_mean_absolute_error: 0.5424\n",
      "Epoch 41/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3317 - mean_absolute_error: 0.4313 - val_loss: 0.4456 - val_mean_absolute_error: 0.5421\n",
      "Epoch 42/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3313 - mean_absolute_error: 0.4296 - val_loss: 0.4519 - val_mean_absolute_error: 0.5471\n",
      "Epoch 43/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3324 - mean_absolute_error: 0.4314 - val_loss: 0.4352 - val_mean_absolute_error: 0.5405\n",
      "Epoch 44/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3395 - mean_absolute_error: 0.4383 - val_loss: 0.4498 - val_mean_absolute_error: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3314 - mean_absolute_error: 0.4296 - val_loss: 0.4499 - val_mean_absolute_error: 0.5463\n",
      "Epoch 46/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3297 - mean_absolute_error: 0.4264 - val_loss: 0.4547 - val_mean_absolute_error: 0.5491\n",
      "Epoch 47/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3303 - mean_absolute_error: 0.4283 - val_loss: 0.4489 - val_mean_absolute_error: 0.5472\n",
      "Epoch 48/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3301 - mean_absolute_error: 0.4264 - val_loss: 0.4549 - val_mean_absolute_error: 0.5500\n",
      "Epoch 49/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3308 - mean_absolute_error: 0.4271 - val_loss: 0.4464 - val_mean_absolute_error: 0.5443\n",
      "Epoch 50/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3305 - mean_absolute_error: 0.4269 - val_loss: 0.4528 - val_mean_absolute_error: 0.5496\n",
      "Epoch 51/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3296 - mean_absolute_error: 0.4250 - val_loss: 0.4579 - val_mean_absolute_error: 0.5505\n",
      "Epoch 52/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3298 - mean_absolute_error: 0.4270 - val_loss: 0.4559 - val_mean_absolute_error: 0.5521\n",
      "Epoch 53/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3294 - mean_absolute_error: 0.4246 - val_loss: 0.4522 - val_mean_absolute_error: 0.5495\n",
      "Epoch 54/100\n",
      "934/934 [==============================] - 0s 44us/step - loss: 0.3294 - mean_absolute_error: 0.4241 - val_loss: 0.4558 - val_mean_absolute_error: 0.5509\n",
      "Epoch 55/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3290 - mean_absolute_error: 0.4234 - val_loss: 0.4535 - val_mean_absolute_error: 0.5505\n",
      "Epoch 56/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 0.3292 - mean_absolute_error: 0.4231 - val_loss: 0.4569 - val_mean_absolute_error: 0.5511\n",
      "Epoch 57/100\n",
      "934/934 [==============================] - 0s 43us/step - loss: 0.3286 - mean_absolute_error: 0.4225 - val_loss: 0.4583 - val_mean_absolute_error: 0.5528\n",
      "Epoch 58/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3286 - mean_absolute_error: 0.4215 - val_loss: 0.4504 - val_mean_absolute_error: 0.5488\n",
      "Epoch 59/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3287 - mean_absolute_error: 0.4234 - val_loss: 0.4626 - val_mean_absolute_error: 0.5547\n",
      "Epoch 60/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.3286 - mean_absolute_error: 0.4213 - val_loss: 0.4588 - val_mean_absolute_error: 0.5538\n",
      "Epoch 61/100\n",
      "934/934 [==============================] - 0s 48us/step - loss: 0.3293 - mean_absolute_error: 0.4236 - val_loss: 0.4575 - val_mean_absolute_error: 0.5503\n",
      "Epoch 62/100\n",
      "934/934 [==============================] - 0s 49us/step - loss: 0.3283 - mean_absolute_error: 0.4227 - val_loss: 0.4595 - val_mean_absolute_error: 0.5520\n",
      "Epoch 63/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3279 - mean_absolute_error: 0.4213 - val_loss: 0.4672 - val_mean_absolute_error: 0.5567\n",
      "Epoch 64/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3284 - mean_absolute_error: 0.4232 - val_loss: 0.4634 - val_mean_absolute_error: 0.5564\n",
      "Epoch 65/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 0.3291 - mean_absolute_error: 0.4255 - val_loss: 0.4648 - val_mean_absolute_error: 0.5582\n",
      "Epoch 66/100\n",
      "934/934 [==============================] - 0s 46us/step - loss: 0.3286 - mean_absolute_error: 0.4230 - val_loss: 0.4522 - val_mean_absolute_error: 0.5512\n",
      "Epoch 67/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3297 - mean_absolute_error: 0.4249 - val_loss: 0.4709 - val_mean_absolute_error: 0.5609\n",
      "Epoch 68/100\n",
      "934/934 [==============================] - 0s 50us/step - loss: 0.3314 - mean_absolute_error: 0.4262 - val_loss: 0.4559 - val_mean_absolute_error: 0.5487\n",
      "Epoch 69/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3301 - mean_absolute_error: 0.4274 - val_loss: 0.4636 - val_mean_absolute_error: 0.5547\n",
      "Epoch 70/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3292 - mean_absolute_error: 0.4240 - val_loss: 0.4583 - val_mean_absolute_error: 0.5530\n",
      "Epoch 71/100\n",
      "934/934 [==============================] - 0s 45us/step - loss: 0.3279 - mean_absolute_error: 0.4221 - val_loss: 0.4673 - val_mean_absolute_error: 0.5557\n",
      "Epoch 72/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3274 - mean_absolute_error: 0.4214 - val_loss: 0.4636 - val_mean_absolute_error: 0.5546\n",
      "Epoch 73/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3264 - mean_absolute_error: 0.4183 - val_loss: 0.4702 - val_mean_absolute_error: 0.5582\n",
      "Epoch 74/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3267 - mean_absolute_error: 0.4205 - val_loss: 0.4579 - val_mean_absolute_error: 0.5531\n",
      "Epoch 75/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3261 - mean_absolute_error: 0.4173 - val_loss: 0.4635 - val_mean_absolute_error: 0.5552\n",
      "Epoch 76/100\n",
      "934/934 [==============================] - 0s 42us/step - loss: 0.3261 - mean_absolute_error: 0.4173 - val_loss: 0.4625 - val_mean_absolute_error: 0.5562\n",
      "Epoch 77/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3260 - mean_absolute_error: 0.4167 - val_loss: 0.4622 - val_mean_absolute_error: 0.5552\n",
      "Epoch 78/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3258 - mean_absolute_error: 0.4162 - val_loss: 0.4611 - val_mean_absolute_error: 0.5552\n",
      "Epoch 79/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3259 - mean_absolute_error: 0.4158 - val_loss: 0.4608 - val_mean_absolute_error: 0.5567\n",
      "Epoch 80/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3274 - mean_absolute_error: 0.4190 - val_loss: 0.4588 - val_mean_absolute_error: 0.5538\n",
      "Epoch 81/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3287 - mean_absolute_error: 0.4236 - val_loss: 0.4547 - val_mean_absolute_error: 0.5526\n",
      "Epoch 82/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3264 - mean_absolute_error: 0.4207 - val_loss: 0.4647 - val_mean_absolute_error: 0.5558\n",
      "Epoch 83/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3262 - mean_absolute_error: 0.4196 - val_loss: 0.4577 - val_mean_absolute_error: 0.5517\n",
      "Epoch 84/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3258 - mean_absolute_error: 0.4189 - val_loss: 0.4678 - val_mean_absolute_error: 0.5580\n",
      "Epoch 85/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3250 - mean_absolute_error: 0.4184 - val_loss: 0.4547 - val_mean_absolute_error: 0.5511\n",
      "Epoch 86/100\n",
      "934/934 [==============================] - 0s 37us/step - loss: 0.3246 - mean_absolute_error: 0.4157 - val_loss: 0.4620 - val_mean_absolute_error: 0.5559\n",
      "Epoch 87/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3248 - mean_absolute_error: 0.4172 - val_loss: 0.4602 - val_mean_absolute_error: 0.5528\n",
      "Epoch 88/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3248 - mean_absolute_error: 0.4170 - val_loss: 0.4676 - val_mean_absolute_error: 0.5586\n",
      "Epoch 89/100\n",
      "934/934 [==============================] - 0s 41us/step - loss: 0.3244 - mean_absolute_error: 0.4156 - val_loss: 0.4655 - val_mean_absolute_error: 0.5583\n",
      "Epoch 90/100\n",
      "934/934 [==============================] - 0s 39us/step - loss: 0.3246 - mean_absolute_error: 0.4153 - val_loss: 0.4642 - val_mean_absolute_error: 0.5564\n",
      "Epoch 91/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3248 - mean_absolute_error: 0.4152 - val_loss: 0.4599 - val_mean_absolute_error: 0.5529\n",
      "Epoch 92/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3245 - mean_absolute_error: 0.4156 - val_loss: 0.4643 - val_mean_absolute_error: 0.5570\n",
      "Epoch 93/100\n",
      "934/934 [==============================] - 0s 47us/step - loss: 0.3239 - mean_absolute_error: 0.4143 - val_loss: 0.4585 - val_mean_absolute_error: 0.5514\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 46us/step - loss: 0.3237 - mean_absolute_error: 0.4136 - val_loss: 0.4635 - val_mean_absolute_error: 0.5557\n",
      "Epoch 95/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3238 - mean_absolute_error: 0.4141 - val_loss: 0.4625 - val_mean_absolute_error: 0.5560\n",
      "Epoch 96/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3241 - mean_absolute_error: 0.4146 - val_loss: 0.4677 - val_mean_absolute_error: 0.5582\n",
      "Epoch 97/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3251 - mean_absolute_error: 0.4178 - val_loss: 0.4658 - val_mean_absolute_error: 0.5564\n",
      "Epoch 98/100\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.3245 - mean_absolute_error: 0.4152 - val_loss: 0.4677 - val_mean_absolute_error: 0.5569\n",
      "Epoch 99/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3239 - mean_absolute_error: 0.4137 - val_loss: 0.4680 - val_mean_absolute_error: 0.5579\n",
      "Epoch 100/100\n",
      "934/934 [==============================] - 0s 40us/step - loss: 0.3239 - mean_absolute_error: 0.4146 - val_loss: 0.4635 - val_mean_absolute_error: 0.5560\n",
      "loss(MSE): 0.8523638607704476\n",
      "MAE: 0.5646119607638006\n"
     ]
    }
   ],
   "source": [
    "#欠損値は削除，object型はOne-Hotとかして，標準化\n",
    "liner_model(X_train_std_2,y_train_std_2,X_val_std_2,y_val_std_2,X_test_std_2,y_test_std_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】MNISTをKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_shape (60000,)\n",
      "y_train_one_hot_shape (48000, 10)\n",
      "y_train_one_hot_dtype: float64\n",
      "X_train_shape (48000, 784)\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータのダウンロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "# 型の変換\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 正解データをOne-Hot表現に\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# trainデータをtrainとvalに分ける\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train_OH, y_val_OH = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "#shapeの確認\n",
    "print('y_train_shape',y_train.shape) # (60000,)\n",
    "print('y_train_one_hot_shape',y_train_OH.shape) # (60000, 10)\n",
    "print('y_train_one_hot_dtype:',y_train_one_hot.dtype) # float64\n",
    "print('X_train_shape',X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 12,730\n",
      "Trainable params: 12,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.4019 - acc: 0.8866 - val_loss: 0.2676 - val_acc: 0.9223\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2350 - acc: 0.9340 - val_loss: 0.2281 - val_acc: 0.9345\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1997 - acc: 0.9428 - val_loss: 0.2042 - val_acc: 0.9428\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1778 - acc: 0.9483 - val_loss: 0.1929 - val_acc: 0.9456\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1631 - acc: 0.9516 - val_loss: 0.1933 - val_acc: 0.9451\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1520 - acc: 0.9549 - val_loss: 0.1865 - val_acc: 0.9480\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.1438 - acc: 0.9568 - val_loss: 0.1861 - val_acc: 0.9462\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1379 - acc: 0.9583 - val_loss: 0.1805 - val_acc: 0.9494\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1329 - acc: 0.9598 - val_loss: 0.1857 - val_acc: 0.9481\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1277 - acc: 0.9615 - val_loss: 0.1782 - val_acc: 0.9506\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.1237 - acc: 0.9627 - val_loss: 0.1751 - val_acc: 0.9515\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.1195 - acc: 0.9635 - val_loss: 0.1808 - val_acc: 0.9506\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.1163 - acc: 0.9641 - val_loss: 0.1846 - val_acc: 0.9503\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.1137 - acc: 0.9647 - val_loss: 0.1762 - val_acc: 0.9517\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1097 - acc: 0.9660 - val_loss: 0.1823 - val_acc: 0.9491\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1080 - acc: 0.9663 - val_loss: 0.1836 - val_acc: 0.9511\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1053 - acc: 0.9675 - val_loss: 0.1817 - val_acc: 0.9511\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.1028 - acc: 0.9687 - val_loss: 0.1782 - val_acc: 0.9519\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1003 - acc: 0.9689 - val_loss: 0.1860 - val_acc: 0.9497\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0987 - acc: 0.9693 - val_loss: 0.1801 - val_acc: 0.9516\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0962 - acc: 0.9698 - val_loss: 0.1900 - val_acc: 0.9481\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0944 - acc: 0.9705 - val_loss: 0.1893 - val_acc: 0.9483\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0922 - acc: 0.9715 - val_loss: 0.1827 - val_acc: 0.9522\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0920 - acc: 0.9714 - val_loss: 0.1794 - val_acc: 0.9541\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0890 - acc: 0.9723 - val_loss: 0.1835 - val_acc: 0.9518\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0882 - acc: 0.9719 - val_loss: 0.1946 - val_acc: 0.9512\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0866 - acc: 0.9727 - val_loss: 0.1862 - val_acc: 0.9525\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0854 - acc: 0.9734 - val_loss: 0.1941 - val_acc: 0.9504\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0845 - acc: 0.9740 - val_loss: 0.1998 - val_acc: 0.9485\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0829 - acc: 0.9739 - val_loss: 0.1882 - val_acc: 0.9529\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0809 - acc: 0.9739 - val_loss: 0.1970 - val_acc: 0.9508\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0807 - acc: 0.9745 - val_loss: 0.2018 - val_acc: 0.9497\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0789 - acc: 0.9749 - val_loss: 0.1960 - val_acc: 0.9514\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0775 - acc: 0.9756 - val_loss: 0.2094 - val_acc: 0.9486\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0766 - acc: 0.9762 - val_loss: 0.2005 - val_acc: 0.9512\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0759 - acc: 0.9751 - val_loss: 0.2110 - val_acc: 0.9482\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.0758 - acc: 0.9761 - val_loss: 0.2059 - val_acc: 0.9495\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0743 - acc: 0.9767 - val_loss: 0.2057 - val_acc: 0.9517\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0738 - acc: 0.9767 - val_loss: 0.2088 - val_acc: 0.9505\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0717 - acc: 0.9770 - val_loss: 0.2069 - val_acc: 0.9510\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0715 - acc: 0.9769 - val_loss: 0.2188 - val_acc: 0.9473\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0698 - acc: 0.9769 - val_loss: 0.2125 - val_acc: 0.9495\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0699 - acc: 0.9777 - val_loss: 0.2143 - val_acc: 0.9507\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0685 - acc: 0.9783 - val_loss: 0.2169 - val_acc: 0.9500\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0677 - acc: 0.9784 - val_loss: 0.2159 - val_acc: 0.9497\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0675 - acc: 0.9784 - val_loss: 0.2169 - val_acc: 0.9502\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.0661 - acc: 0.9789 - val_loss: 0.2322 - val_acc: 0.9459\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0659 - acc: 0.9787 - val_loss: 0.2257 - val_acc: 0.9492\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0644 - acc: 0.9794 - val_loss: 0.2256 - val_acc: 0.9505\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0643 - acc: 0.9798 - val_loss: 0.2297 - val_acc: 0.9488\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0633 - acc: 0.9800 - val_loss: 0.2387 - val_acc: 0.9480\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0624 - acc: 0.9800 - val_loss: 0.2350 - val_acc: 0.9494\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0623 - acc: 0.9797 - val_loss: 0.2333 - val_acc: 0.9507\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0613 - acc: 0.9799 - val_loss: 0.2369 - val_acc: 0.9488\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0612 - acc: 0.9803 - val_loss: 0.2403 - val_acc: 0.9472\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0606 - acc: 0.9806 - val_loss: 0.2450 - val_acc: 0.9483\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0599 - acc: 0.9806 - val_loss: 0.2436 - val_acc: 0.9497\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0589 - acc: 0.9804 - val_loss: 0.2422 - val_acc: 0.9484\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0586 - acc: 0.9812 - val_loss: 0.2417 - val_acc: 0.9495\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0583 - acc: 0.9810 - val_loss: 0.2482 - val_acc: 0.9469\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0567 - acc: 0.9820 - val_loss: 0.2478 - val_acc: 0.9498\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0568 - acc: 0.9819 - val_loss: 0.2463 - val_acc: 0.9500\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0555 - acc: 0.9820 - val_loss: 0.2553 - val_acc: 0.9476\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0548 - acc: 0.9826 - val_loss: 0.2557 - val_acc: 0.9472\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0549 - acc: 0.9831 - val_loss: 0.2583 - val_acc: 0.9467\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0548 - acc: 0.9827 - val_loss: 0.2570 - val_acc: 0.9475\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0529 - acc: 0.9827 - val_loss: 0.2624 - val_acc: 0.9474\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.0536 - acc: 0.9830 - val_loss: 0.2682 - val_acc: 0.9468\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0529 - acc: 0.9834 - val_loss: 0.2750 - val_acc: 0.9450\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0524 - acc: 0.9826 - val_loss: 0.2715 - val_acc: 0.9471\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0511 - acc: 0.9833 - val_loss: 0.2698 - val_acc: 0.9455\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0526 - acc: 0.9829 - val_loss: 0.2721 - val_acc: 0.9460\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0506 - acc: 0.9842 - val_loss: 0.2705 - val_acc: 0.9482\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0508 - acc: 0.9833 - val_loss: 0.2677 - val_acc: 0.9483\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0497 - acc: 0.9837 - val_loss: 0.2791 - val_acc: 0.9478\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0495 - acc: 0.9840 - val_loss: 0.2831 - val_acc: 0.9457\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0492 - acc: 0.9844 - val_loss: 0.2800 - val_acc: 0.9477\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0482 - acc: 0.9844 - val_loss: 0.2820 - val_acc: 0.9482\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0485 - acc: 0.9842 - val_loss: 0.2911 - val_acc: 0.9456\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0475 - acc: 0.9849 - val_loss: 0.2897 - val_acc: 0.9462\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0481 - acc: 0.9840 - val_loss: 0.2884 - val_acc: 0.9456\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.0468 - acc: 0.9849 - val_loss: 0.2968 - val_acc: 0.9452\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0473 - acc: 0.9847 - val_loss: 0.2922 - val_acc: 0.9453\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0452 - acc: 0.9852 - val_loss: 0.2956 - val_acc: 0.9457\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0460 - acc: 0.9851 - val_loss: 0.2977 - val_acc: 0.9467\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0449 - acc: 0.9855 - val_loss: 0.3017 - val_acc: 0.9458\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0449 - acc: 0.9858 - val_loss: 0.2989 - val_acc: 0.9471\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0443 - acc: 0.9860 - val_loss: 0.3029 - val_acc: 0.9447\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0447 - acc: 0.9854 - val_loss: 0.3082 - val_acc: 0.9458\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0439 - acc: 0.9851 - val_loss: 0.3050 - val_acc: 0.9460\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0433 - acc: 0.9860 - val_loss: 0.3123 - val_acc: 0.9430\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0440 - acc: 0.9861 - val_loss: 0.3016 - val_acc: 0.9457\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0425 - acc: 0.9861 - val_loss: 0.3070 - val_acc: 0.9462\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0415 - acc: 0.9863 - val_loss: 0.3165 - val_acc: 0.9435\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0429 - acc: 0.9860 - val_loss: 0.3185 - val_acc: 0.9459\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.0414 - acc: 0.9864 - val_loss: 0.3215 - val_acc: 0.9442\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0420 - acc: 0.9861 - val_loss: 0.3162 - val_acc: 0.9450\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0393 - acc: 0.9874 - val_loss: 0.3218 - val_acc: 0.9444\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0419 - acc: 0.9861 - val_loss: 0.3230 - val_acc: 0.9427\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0411 - acc: 0.9868 - val_loss: 0.3185 - val_acc: 0.9457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238d1e27288>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "lr = 0.001\n",
    "batch_size = 20\n",
    "num_epochs = 100\n",
    "\n",
    "n_input = X_train.shape[1]     #入力層のノード数\n",
    "n_output = y_train_OH.shape[1]    #出力層のノード数\n",
    "print(n_input)\n",
    "\n",
    "K.clear_session()#レイヤの通し番号がつながるの防止\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=n_input, kernel_initializer=he_normal()))#入力層→隠れ層\n",
    "model.add(Activation('relu')) #活性化層\n",
    "model.add(Dense(n_output))    #隠れ層→出力層\n",
    "model.add(Activation('softmax')) \n",
    "\n",
    "model.summary()\n",
    "#コンパイル（多値なのでlossはcategorical_crossentropy）\n",
    "model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#学習\n",
    "model.fit(X_train,y_train_OH,\n",
    "          batch_size=batch_size,epochs=num_epochs,verbose=1,\n",
    "          validation_data=(X_val, y_val_OH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(MSE): 0.2952535346505465\n",
      "MAE: 0.9473\n"
     ]
    }
   ],
   "source": [
    "# 推定\n",
    "score = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print('loss(MSE):', score[0])\n",
    "print('accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】（アドバンス課題）PyTorchへの書き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
